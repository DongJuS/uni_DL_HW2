{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069b4d7c-36e4-4ee2-bc8c-66b7da8d7d99",
   "metadata": {},
   "source": [
    "# 요구사항 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a3559-efb2-442f-b463-47eece214eb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 1: Imports and Dataset Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7677640a-9686-4957-b99e-06fc71035c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and Dataset Class Definitions\n",
    "import os # os: 파일 및 디렉토리 경로 조작에 사용\n",
    "import torch # torch: PyTorch 라이브러리, 딥러닝을 위한 주요 패키지\n",
    "import pandas as pd # pandas: 데이터프레임 구조를 사용하여 데이터 처리 및 분석\n",
    "from torch import nn, optim # nn, optim: PyTorch의 신경망 계층 및 옵티마이저 정의\n",
    "from torch.utils.data import Dataset, DataLoader, random_split # Dataset, DataLoader, random_split: PyTorch에서 데이터를 로드하고, 배치로 나누고, 훈련/검증 데이터셋을 분할하는 데 사용\n",
    "from datetime import datetime # datetime: 현재 시간 및 날짜 처리\n",
    "import wandb # wandb: 학습 과정을 추적하고 시각화하는 WandB 라이브러리\n",
    "import argparse # argparse: 명령줄 인자 파싱\n",
    "from sklearn.preprocessing import StandardScaler # StandardScaler: scikit-learn에서 제공하는 표준화 도구로, 데이터의 평균을 0으로, 분산을 1로 조정\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # accuracy_score, precision_score, recall_score, f1_score: scikit-learn에서 제공하는 성능 평가 메트릭\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf91bd-4610-4d78-9c1b-725f0cb67e99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 2: # TitanicDataset Class for Train and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94716d6f-2894-4581-8549-444a0aceed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TitanicDataset Class for Train and Validation Data\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # 초기화 메서드: 입력 데이터(X)와 타겟 데이터(y)를 텐서로 변환하여 저장\n",
    "        self.X = torch.FloatTensor(X)  # 입력 데이터를 FloatTensor로 변환\n",
    "        self.y = torch.LongTensor(y)   # 타겟 데이터를 LongTensor로 변환 (분류 작업에 적합)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이를 반환하는 메서드\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스(idx)에 해당하는 데이터 포인트를 반환하는 메서드\n",
    "        return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    \n",
    "    def __str__(self):\n",
    "        # 데이터셋의 정보를 문자열로 반환하는 메서드\n",
    "        return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1942ad-b7d7-47b1-a641-dc97209d6000",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 3: TitanicTestDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c537923-2854-4e34-bf4c-e7d10f1920ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X, passenger_ids):\n",
    "        # 초기화 메서드: 입력 데이터(X)를 FloatTensor로 변환하고, PassengerId를 저장\n",
    "        self.X = torch.FloatTensor(X)  # 입력 데이터를 FloatTensor로 변환\n",
    "        self.passenger_ids = passenger_ids  # PassengerId 리스트 저장\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이를 반환하는 메서드\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스(idx)에 해당하는 데이터 포인트를 반환하는 메서드\n",
    "        feature = self.X[idx]  # 입력 데이터\n",
    "        passenger_id = self.passenger_ids[idx]  # 해당 인덱스의 PassengerId\n",
    "        return {'input': feature, 'PassengerId': passenger_id}  # 입력 특징과 PassengerId 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0e341-a7d3-42d9-8151-43d71190bd9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 4: get_preprocessed_dataset_1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33191553-c6b6-4793-a871-4ee474db2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(df):\n",
    "    # 성별 인코딩 (male=0, female=1)\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # 성별을 숫자로 인코딩하여 모델이 처리할 수 있도록 변환\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_2(df):\n",
    "    # 결측값 처리 (Age의 결측값을 평균으로 채움)\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].mean())  # Age의 결측값을 평균으로 대체\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_3(df):\n",
    "    # 결측값 처리 (Fare의 결측값을 평균으로 채움)\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())  # Fare의 결측값을 평균으로 대체\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_4(df):\n",
    "    # 불필요한 열 제거, 단 PassengerId는 제거하지 않음\n",
    "    columns_to_drop = ['Name', 'Ticket', 'Cabin', 'Embarked']  # 분석에 불필요한 열 목록\n",
    "    df = df.drop(columns=columns_to_drop, axis=1, errors='ignore')  # 지정된 열을 데이터프레임에서 제거\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_5(df):\n",
    "    # Pclass를 범주형에서 연속형으로 처리\n",
    "    df['Pclass'] = df['Pclass'].astype(float)  # 클래스 열을 float로 변환하여 연속형 변수로 사용\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_6(df):\n",
    "    # 정규화 (필요한 경우)\n",
    "    scaler = StandardScaler()  # 데이터의 분포를 표준화하여 평균을 0, 표준편차를 1로 조정\n",
    "    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']  # 정규화할 특징 목록\n",
    "    df[features] = scaler.fit_transform(df[features])  # 특징들을 정규화하여 모델의 수렴 속도를 높임\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6353a-3d7e-46ac-9a50-9923a3dd3ae3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 5 : get_preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a328c0a5-c8e5-4284-81e9-76773735ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Preprocessing\n",
    "def get_preprocessed_dataset():\n",
    "    # 현재 작업 디렉토리 경로를 기준으로 데이터 파일 경로 설정\n",
    "    CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    \n",
    "    # 훈련 데이터와 테스트 데이터를 각각 로드하여 DataFrame 형태로 저장\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # 훈련 및 테스트 데이터를 하나의 DataFrame으로 병합 (전처리를 한 번에 수행하기 위함)\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    \n",
    "    # 전처리 함수들을 순차적으로 호출하여 데이터셋 전처리 수행\n",
    "    all_df = get_preprocessed_dataset_1(all_df)  # 성별 인코딩\n",
    "    all_df = get_preprocessed_dataset_2(all_df)  # Age 결측값 처리\n",
    "    all_df = get_preprocessed_dataset_3(all_df)  # Fare 결측값 처리\n",
    "    all_df = get_preprocessed_dataset_4(all_df)  # 불필요한 열 제거\n",
    "    all_df = get_preprocessed_dataset_5(all_df)  # Pclass 변환\n",
    "    all_df = get_preprocessed_dataset_6(all_df)  # 정규화\n",
    "    \n",
    "    return all_df  # 전처리된 데이터프레임 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45ddd0-5acf-45d2-aca4-b8f705e0dd97",
   "metadata": {},
   "source": [
    "## Section 6 : Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b00a749a-4223-4c23-986d-4fcf66141d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Model Definition version 2\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        # 신경망 계층 정의: ELU 활성화 함수와 Dropout을 통해 과적합 방지\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 256),  # 첫 번째 은닉층, 입력 크기에서 256개의 뉴런으로 연결\n",
    "            nn.ELU(),                 # ELU 활성화 함수, 비선형성을 추가해 복잡한 패턴 학습\n",
    "           # nn.Dropout(0.3),          # 30%의 뉴런을 무작위로 비활성화하여 과적합 방지\n",
    "            nn.Linear(256, 64),       # 두 번째 은닉층, 64개의 뉴런으로 연결\n",
    "            nn.ELU(),                 # ELU 활성화 함수\n",
    "           # nn.Dropout(0.3),          # 30%의 Dropout 추가\n",
    "            nn.Linear(64, n_output),  # 출력층, n_output 클래스 수로 연결\n",
    "            nn.Sigmoid()          # 이진 분류를 위해 Sigmoid 활성화 추가\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 순전파 정의: 입력 x를 신경망 계층에 통과시켜 결과를 반환\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa50e25-3037-471b-8ad3-4f1a13995e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Training and Evaluation Loops_v2_wandB에 추가하기\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs  # WandB 설정에서 에포크 수 가져오기\n",
    "    loss_fn = nn.CrossEntropyLoss()  # 다중 클래스 분류 손실 함수 정의\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Training Loop\n",
    "        model.train()  # 모델을 학습 모드로 설정\n",
    "        train_loss = 0.0  # 훈련 손실 초기화\n",
    "        for batch in train_data_loader:\n",
    "            inputs, targets = batch['input'], batch['target']  # 입력과 타겟 분리\n",
    "            outputs = model(inputs)  # 모델을 통한 예측\n",
    "            loss = loss_fn(outputs, targets)  # 손실 계산\n",
    "            train_loss += loss.item()  # 배치 손실 합산\n",
    "            \n",
    "            optimizer.zero_grad()  # 기울기 초기화\n",
    "            loss.backward()  # 역전파로 기울기 계산\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "        train_loss /= len(train_data_loader)  # 에포크별 평균 훈련 손실 계산\n",
    "\n",
    "        # Validation Loss 계산\n",
    "        model.eval()  # 모델을 평가 모드로 설정\n",
    "        validation_loss = 0.0  # 검증 손실 초기화\n",
    "        with torch.no_grad():  # 검증 시 기울기 계산 방지\n",
    "            for batch in validation_data_loader:\n",
    "                inputs, targets = batch['input'], batch['target']\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(validation_data_loader)  # 에포크별 평균 검증 손실 계산\n",
    "\n",
    "        # WandB에 학습 및 검증 손실 기록\n",
    "        wandb.log({\"Training Loss\": train_loss, \"Validation Loss\": validation_loss, \"Epoch\": epoch})\n",
    "\n",
    "        # 에포크별 훈련 및 검증 손실 출력\n",
    "        print(f\"Epoch {epoch} - Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afc5ae-ec8d-4fc9-b50e-52f84b29563c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 7 : predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95ffb94e-4160-4ec7-af6e-8285e3aa26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_test version 5\n",
    "# Section 5: Model Testing and Prediction\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Model Testing and Prediction\n",
    "def predict_test(model, test_data_loader, output_path=\"test_predictions_ELU_testing_almost_final.csv\"):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    predictions = []  # 예측 결과를 저장할 리스트\n",
    "    passenger_ids = []  # PassengerId 저장을 위한 리스트\n",
    "\n",
    "    # 예측 수행\n",
    "    with torch.no_grad():  # 평가 시에는 기울기 계산 비활성화\n",
    "        for batch in test_data_loader:\n",
    "            inputs = batch['input']  # 입력 데이터\n",
    "            passenger_ids_batch = batch['PassengerId']  # PassengerId 가져오기\n",
    "            outputs = model(inputs)  # 모델을 통해 예측 수행\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)  # 예측 확률 중 가장 높은 값을 가진 클래스로 변환\n",
    "            \n",
    "            # 예측 값과 PassengerId 저장\n",
    "            predictions.extend(predicted_labels.cpu().numpy())  # Tensor에서 numpy로 변환하여 리스트에 추가\n",
    "            passenger_ids.extend(passenger_ids_batch.cpu().numpy())  # Tensor에서 numpy로 변환하여 리스트에 추가\n",
    "\n",
    "    # 예측 결과를 DataFrame으로 변환\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"PassengerId\": passenger_ids,  # PassengerId 열\n",
    "        \"Survived\": predictions        # 예측된 Survived 값\n",
    "    })\n",
    "    \n",
    "    # DataFrame을 CSV 파일로 저장\n",
    "    preds_df.to_csv(output_path, index=False)  # 지정된 경로에 CSV 파일로 저장\n",
    "    print(f\"Test predictions saved to {os.path.abspath(output_path)}\")  # 파일의 절대 경로 출력\n",
    "\n",
    "    return predictions  # 예측 값 리스트 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64aa37b-1479-40e4-b2b3-1195882324e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 8 : main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9751aef4-6d46-4d32-b235-236e58270c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3\n",
    "def main(args):\n",
    "    # WandB 프로젝트 초기화 및 설정 로깅\n",
    "    wandb.init(project=\"titanic_model_training\", config={\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"learning_rate\": args.learning_rate\n",
    "    })\n",
    "\n",
    "    # 전처리된 데이터셋 로드\n",
    "    all_df = get_preprocessed_dataset()\n",
    "    \n",
    "    # 훈련, 검증, 테스트 데이터셋 분리\n",
    "    train_df = all_df[all_df['Survived'].notna()]  # 생존 데이터(Survived) 존재 행을 훈련용으로 사용\n",
    "    test_df = all_df[all_df['Survived'].isna()]    # 생존 데이터가 없는 행을 테스트용으로 사용\n",
    "    \n",
    "    # 특징(X)와 타겟(y) 분리\n",
    "    X_train = train_df.drop(columns=['Survived']).values  # 타겟 열 제거하여 입력 데이터만 남김\n",
    "    y_train = train_df['Survived'].values                 # 타겟 값만 분리\n",
    "    X_test = test_df.drop(columns=['Survived']).values    # 테스트 데이터에서 타겟 열 제거\n",
    "    passenger_ids_test = test_df['PassengerId'].values    # 테스트 데이터에서 PassengerId 가져오기\n",
    "\n",
    "    # Dataset 생성\n",
    "    full_train_dataset = TitanicDataset(X_train, y_train)\n",
    "    \n",
    "    # 훈련/검증 데이터 분리 (80% 훈련, 20% 검증)\n",
    "    train_size = int(0.8 * len(full_train_dataset))  # 전체 훈련 데이터의 80%\n",
    "    val_size = len(full_train_dataset) - train_size  # 나머지 20%를 검증에 사용\n",
    "    train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # 테스트 데이터셋에 PassengerId 포함\n",
    "    test_dataset = TitanicTestDataset(X_test, passenger_ids_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)       # 훈련 데이터 로더\n",
    "    validation_data_loader = DataLoader(validation_dataset, batch_size=args.batch_size, shuffle=True)  # 검증 데이터 로더\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=len(test_dataset))                     # 테스트 데이터 로더\n",
    "\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model = MyModel(n_input=X_train.shape[1], n_output=2)  # 입력 피처 수와 이진 분류에 맞춰 모델 정의\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)  # Adam 옵티마이저로 학습 설정\n",
    "\n",
    "    # 훈련 루프 실행\n",
    "    training_loop(model, optimizer, train_data_loader, validation_data_loader)\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 수행 및 결과 출력\n",
    "    test_predictions = predict_test(model, test_data_loader)\n",
    "    print(f\"Test predictions: {test_predictions}\")\n",
    "\n",
    "    wandb.finish()  # WandB 세션 종료\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50817c4-4c3e-45cc-ac8d-4338acb8bf76",
   "metadata": {},
   "source": [
    "## Section 9: Argument Parsing and Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2becb91-9703-423d-8f86-55fdc2c480f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: -ddj127 (-ddj127-korea-university-of-technology-and-education). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\wandb\\run-20241025_174645-3556bhjg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/3556bhjg' target=\"_blank\">quiet-meadow-40</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/3556bhjg' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/3556bhjg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.7102, Validation Loss: 0.6849\n",
      "Epoch 2 - Training Loss: 0.6784, Validation Loss: 0.6863\n",
      "Epoch 3 - Training Loss: 0.6774, Validation Loss: 0.6910\n",
      "Epoch 4 - Training Loss: 0.6963, Validation Loss: 0.6867\n",
      "Epoch 5 - Training Loss: 0.6863, Validation Loss: 0.6970\n",
      "Epoch 6 - Training Loss: 0.6951, Validation Loss: 0.6859\n",
      "Epoch 7 - Training Loss: 0.7032, Validation Loss: 0.6892\n",
      "Epoch 8 - Training Loss: 0.7209, Validation Loss: 0.6838\n",
      "Epoch 9 - Training Loss: 0.6866, Validation Loss: 0.6849\n",
      "Epoch 10 - Training Loss: 0.6952, Validation Loss: 0.6868\n",
      "Epoch 11 - Training Loss: 0.6932, Validation Loss: 0.6818\n",
      "Epoch 12 - Training Loss: 0.6669, Validation Loss: 0.6871\n",
      "Epoch 13 - Training Loss: 0.6827, Validation Loss: 0.6829\n",
      "Epoch 14 - Training Loss: 0.6933, Validation Loss: 0.6858\n",
      "Epoch 15 - Training Loss: 0.6837, Validation Loss: 0.6885\n",
      "Epoch 16 - Training Loss: 0.7021, Validation Loss: 0.6831\n",
      "Epoch 17 - Training Loss: 0.6994, Validation Loss: 0.6838\n",
      "Epoch 18 - Training Loss: 0.6897, Validation Loss: 0.6759\n",
      "Epoch 19 - Training Loss: 0.6726, Validation Loss: 0.6779\n",
      "Epoch 20 - Training Loss: 0.6902, Validation Loss: 0.6798\n",
      "Epoch 21 - Training Loss: 0.6650, Validation Loss: 0.6852\n",
      "Epoch 22 - Training Loss: 0.7017, Validation Loss: 0.6822\n",
      "Epoch 23 - Training Loss: 0.7103, Validation Loss: 0.6769\n",
      "Epoch 24 - Training Loss: 0.6991, Validation Loss: 0.6816\n",
      "Epoch 25 - Training Loss: 0.7080, Validation Loss: 0.6907\n",
      "Epoch 26 - Training Loss: 0.7003, Validation Loss: 0.6815\n",
      "Epoch 27 - Training Loss: 0.7008, Validation Loss: 0.6844\n",
      "Epoch 28 - Training Loss: 0.6911, Validation Loss: 0.6841\n",
      "Epoch 29 - Training Loss: 0.6726, Validation Loss: 0.6774\n",
      "Epoch 30 - Training Loss: 0.6817, Validation Loss: 0.6814\n",
      "Epoch 31 - Training Loss: 0.6733, Validation Loss: 0.6775\n",
      "Epoch 32 - Training Loss: 0.6919, Validation Loss: 0.6879\n",
      "Epoch 33 - Training Loss: 0.6964, Validation Loss: 0.6773\n",
      "Epoch 34 - Training Loss: 0.6982, Validation Loss: 0.6730\n",
      "Epoch 35 - Training Loss: 0.6701, Validation Loss: 0.6744\n",
      "Epoch 36 - Training Loss: 0.7006, Validation Loss: 0.6811\n",
      "Epoch 37 - Training Loss: 0.6886, Validation Loss: 0.6861\n",
      "Epoch 38 - Training Loss: 0.6742, Validation Loss: 0.6800\n",
      "Epoch 39 - Training Loss: 0.7015, Validation Loss: 0.6812\n",
      "Epoch 40 - Training Loss: 0.6826, Validation Loss: 0.6818\n",
      "Epoch 41 - Training Loss: 0.7090, Validation Loss: 0.6786\n",
      "Epoch 42 - Training Loss: 0.6732, Validation Loss: 0.6784\n",
      "Epoch 43 - Training Loss: 0.6818, Validation Loss: 0.6784\n",
      "Epoch 44 - Training Loss: 0.6898, Validation Loss: 0.6838\n",
      "Epoch 45 - Training Loss: 0.6893, Validation Loss: 0.6815\n",
      "Epoch 46 - Training Loss: 0.6908, Validation Loss: 0.6830\n",
      "Epoch 47 - Training Loss: 0.7000, Validation Loss: 0.6788\n",
      "Epoch 48 - Training Loss: 0.6718, Validation Loss: 0.6852\n",
      "Epoch 49 - Training Loss: 0.6905, Validation Loss: 0.6811\n",
      "Epoch 50 - Training Loss: 0.6904, Validation Loss: 0.6812\n",
      "Epoch 51 - Training Loss: 0.6982, Validation Loss: 0.6782\n",
      "Epoch 52 - Training Loss: 0.6903, Validation Loss: 0.6795\n",
      "Epoch 53 - Training Loss: 0.6909, Validation Loss: 0.6770\n",
      "Epoch 54 - Training Loss: 0.6807, Validation Loss: 0.6724\n",
      "Epoch 55 - Training Loss: 0.6793, Validation Loss: 0.6770\n",
      "Epoch 56 - Training Loss: 0.6812, Validation Loss: 0.6726\n",
      "Epoch 57 - Training Loss: 0.6985, Validation Loss: 0.6808\n",
      "Epoch 58 - Training Loss: 0.6858, Validation Loss: 0.6783\n",
      "Epoch 59 - Training Loss: 0.6991, Validation Loss: 0.6861\n",
      "Epoch 60 - Training Loss: 0.7070, Validation Loss: 0.6823\n",
      "Epoch 61 - Training Loss: 0.6976, Validation Loss: 0.6847\n",
      "Epoch 62 - Training Loss: 0.6795, Validation Loss: 0.6835\n",
      "Epoch 63 - Training Loss: 0.6845, Validation Loss: 0.6802\n",
      "Epoch 64 - Training Loss: 0.7030, Validation Loss: 0.6721\n",
      "Epoch 65 - Training Loss: 0.6843, Validation Loss: 0.6853\n",
      "Epoch 66 - Training Loss: 0.6834, Validation Loss: 0.6773\n",
      "Epoch 67 - Training Loss: 0.6873, Validation Loss: 0.6907\n",
      "Epoch 68 - Training Loss: 0.6716, Validation Loss: 0.6875\n",
      "Epoch 69 - Training Loss: 0.6999, Validation Loss: 0.6828\n",
      "Epoch 70 - Training Loss: 0.6986, Validation Loss: 0.6826\n",
      "Epoch 71 - Training Loss: 0.7082, Validation Loss: 0.6820\n",
      "Epoch 72 - Training Loss: 0.6714, Validation Loss: 0.6865\n",
      "Epoch 73 - Training Loss: 0.6897, Validation Loss: 0.6877\n",
      "Epoch 74 - Training Loss: 0.6895, Validation Loss: 0.6831\n",
      "Epoch 75 - Training Loss: 0.6876, Validation Loss: 0.6770\n",
      "Epoch 76 - Training Loss: 0.6853, Validation Loss: 0.6759\n",
      "Epoch 77 - Training Loss: 0.6965, Validation Loss: 0.6729\n",
      "Epoch 78 - Training Loss: 0.7052, Validation Loss: 0.6867\n",
      "Epoch 79 - Training Loss: 0.6947, Validation Loss: 0.6758\n",
      "Epoch 80 - Training Loss: 0.7087, Validation Loss: 0.6714\n",
      "Epoch 81 - Training Loss: 0.6829, Validation Loss: 0.6870\n",
      "Epoch 82 - Training Loss: 0.6796, Validation Loss: 0.6869\n",
      "Epoch 83 - Training Loss: 0.6526, Validation Loss: 0.6874\n",
      "Epoch 84 - Training Loss: 0.6742, Validation Loss: 0.6859\n",
      "Epoch 85 - Training Loss: 0.6666, Validation Loss: 0.6819\n",
      "Epoch 86 - Training Loss: 0.6817, Validation Loss: 0.6857\n",
      "Epoch 87 - Training Loss: 0.6910, Validation Loss: 0.6756\n",
      "Epoch 88 - Training Loss: 0.6790, Validation Loss: 0.6847\n",
      "Epoch 89 - Training Loss: 0.6785, Validation Loss: 0.6918\n",
      "Epoch 90 - Training Loss: 0.6925, Validation Loss: 0.6795\n",
      "Epoch 91 - Training Loss: 0.6724, Validation Loss: 0.6913\n",
      "Epoch 92 - Training Loss: 0.6599, Validation Loss: 0.6781\n",
      "Epoch 93 - Training Loss: 0.6720, Validation Loss: 0.6861\n",
      "Epoch 94 - Training Loss: 0.6938, Validation Loss: 0.6938\n",
      "Epoch 95 - Training Loss: 0.9050, Validation Loss: 0.9340\n",
      "Epoch 96 - Training Loss: 0.9305, Validation Loss: 0.9287\n",
      "Epoch 97 - Training Loss: 0.9305, Validation Loss: 0.9273\n",
      "Epoch 98 - Training Loss: 0.9089, Validation Loss: 0.9260\n",
      "Epoch 99 - Training Loss: 0.9137, Validation Loss: 0.9306\n",
      "Epoch 100 - Training Loss: 0.8778, Validation Loss: 0.9377\n",
      "Epoch 101 - Training Loss: 0.7484, Validation Loss: 0.6823\n",
      "Epoch 102 - Training Loss: 0.6947, Validation Loss: 0.6940\n",
      "Epoch 103 - Training Loss: 0.7116, Validation Loss: 0.6901\n",
      "Epoch 104 - Training Loss: 0.6948, Validation Loss: 0.6914\n",
      "Epoch 105 - Training Loss: 0.7039, Validation Loss: 0.6874\n",
      "Epoch 106 - Training Loss: 0.6856, Validation Loss: 0.6835\n",
      "Epoch 107 - Training Loss: 0.6948, Validation Loss: 0.6901\n",
      "Epoch 108 - Training Loss: 0.7026, Validation Loss: 0.6914\n",
      "Epoch 109 - Training Loss: 0.6931, Validation Loss: 0.6888\n",
      "Epoch 110 - Training Loss: 0.6932, Validation Loss: 0.6883\n",
      "Epoch 111 - Training Loss: 0.6748, Validation Loss: 0.6859\n",
      "Epoch 112 - Training Loss: 0.6844, Validation Loss: 0.6806\n",
      "Epoch 113 - Training Loss: 0.7110, Validation Loss: 0.6864\n",
      "Epoch 114 - Training Loss: 0.6844, Validation Loss: 0.6849\n",
      "Epoch 115 - Training Loss: 0.6897, Validation Loss: 0.6849\n",
      "Epoch 116 - Training Loss: 0.6726, Validation Loss: 0.6836\n",
      "Epoch 117 - Training Loss: 0.6903, Validation Loss: 0.6836\n",
      "Epoch 118 - Training Loss: 0.6896, Validation Loss: 0.6796\n",
      "Epoch 119 - Training Loss: 0.6987, Validation Loss: 0.6796\n",
      "Epoch 120 - Training Loss: 0.6999, Validation Loss: 0.6822\n",
      "Epoch 121 - Training Loss: 0.6895, Validation Loss: 0.6822\n",
      "Epoch 122 - Training Loss: 0.7066, Validation Loss: 0.6849\n",
      "Epoch 123 - Training Loss: 0.6987, Validation Loss: 0.6836\n",
      "Epoch 124 - Training Loss: 0.6909, Validation Loss: 0.6796\n",
      "Epoch 125 - Training Loss: 0.6973, Validation Loss: 0.6849\n",
      "Epoch 126 - Training Loss: 0.6805, Validation Loss: 0.6849\n",
      "Epoch 127 - Training Loss: 0.7000, Validation Loss: 0.6849\n",
      "Epoch 128 - Training Loss: 0.6999, Validation Loss: 0.6862\n",
      "Epoch 129 - Training Loss: 0.6806, Validation Loss: 0.6770\n",
      "Epoch 130 - Training Loss: 0.7000, Validation Loss: 0.6823\n",
      "Epoch 131 - Training Loss: 0.6896, Validation Loss: 0.6797\n",
      "Epoch 132 - Training Loss: 0.6883, Validation Loss: 0.6811\n",
      "Epoch 133 - Training Loss: 0.6907, Validation Loss: 0.6833\n",
      "Epoch 134 - Training Loss: 0.7065, Validation Loss: 0.6783\n",
      "Epoch 135 - Training Loss: 0.6896, Validation Loss: 0.6903\n",
      "Epoch 136 - Training Loss: 0.6635, Validation Loss: 0.6833\n",
      "Epoch 137 - Training Loss: 0.6806, Validation Loss: 0.6896\n",
      "Epoch 138 - Training Loss: 0.6790, Validation Loss: 0.6771\n",
      "Epoch 139 - Training Loss: 0.6791, Validation Loss: 0.6849\n",
      "Epoch 140 - Training Loss: 0.6974, Validation Loss: 0.6809\n",
      "Epoch 141 - Training Loss: 0.6861, Validation Loss: 0.6822\n",
      "Epoch 142 - Training Loss: 0.6937, Validation Loss: 0.6808\n",
      "Epoch 143 - Training Loss: 0.6735, Validation Loss: 0.6881\n",
      "Epoch 144 - Training Loss: 0.6838, Validation Loss: 0.6875\n",
      "Epoch 145 - Training Loss: 0.6895, Validation Loss: 0.6848\n",
      "Epoch 146 - Training Loss: 0.6805, Validation Loss: 0.6865\n",
      "Epoch 147 - Training Loss: 0.6815, Validation Loss: 0.6845\n",
      "Epoch 148 - Training Loss: 0.7086, Validation Loss: 0.6854\n",
      "Epoch 149 - Training Loss: 0.6794, Validation Loss: 0.6860\n",
      "Epoch 150 - Training Loss: 0.6974, Validation Loss: 0.6836\n",
      "Epoch 151 - Training Loss: 0.6794, Validation Loss: 0.6796\n",
      "Epoch 152 - Training Loss: 0.6972, Validation Loss: 0.6862\n",
      "Epoch 153 - Training Loss: 0.6974, Validation Loss: 0.6822\n",
      "Epoch 154 - Training Loss: 0.6709, Validation Loss: 0.6836\n",
      "Epoch 155 - Training Loss: 0.6878, Validation Loss: 0.6849\n",
      "Epoch 156 - Training Loss: 0.7121, Validation Loss: 0.6809\n",
      "Epoch 157 - Training Loss: 0.7063, Validation Loss: 0.6889\n",
      "Epoch 158 - Training Loss: 0.7027, Validation Loss: 0.6822\n",
      "Epoch 159 - Training Loss: 0.6779, Validation Loss: 0.6833\n",
      "Epoch 160 - Training Loss: 0.6686, Validation Loss: 0.6848\n",
      "Epoch 161 - Training Loss: 0.6856, Validation Loss: 0.6810\n",
      "Epoch 162 - Training Loss: 0.6778, Validation Loss: 0.6872\n",
      "Epoch 163 - Training Loss: 0.6862, Validation Loss: 0.6896\n",
      "Epoch 164 - Training Loss: 0.7004, Validation Loss: 0.6927\n",
      "Epoch 165 - Training Loss: 0.6641, Validation Loss: 0.6901\n",
      "Epoch 166 - Training Loss: 0.6883, Validation Loss: 0.6914\n",
      "Epoch 167 - Training Loss: 0.6903, Validation Loss: 0.6770\n",
      "Epoch 168 - Training Loss: 0.7140, Validation Loss: 0.6901\n",
      "Epoch 169 - Training Loss: 0.6734, Validation Loss: 0.6902\n",
      "Epoch 170 - Training Loss: 0.6900, Validation Loss: 0.6862\n",
      "Epoch 171 - Training Loss: 0.6883, Validation Loss: 0.6822\n",
      "Epoch 172 - Training Loss: 0.6727, Validation Loss: 0.6723\n",
      "Epoch 173 - Training Loss: 0.7026, Validation Loss: 0.6837\n",
      "Epoch 174 - Training Loss: 0.7051, Validation Loss: 0.6820\n",
      "Epoch 175 - Training Loss: 0.6732, Validation Loss: 0.6804\n",
      "Epoch 176 - Training Loss: 0.7085, Validation Loss: 0.6866\n",
      "Epoch 177 - Training Loss: 0.6764, Validation Loss: 0.6759\n",
      "Epoch 178 - Training Loss: 0.6948, Validation Loss: 0.6786\n",
      "Epoch 179 - Training Loss: 0.7064, Validation Loss: 0.6721\n",
      "Epoch 180 - Training Loss: 0.6607, Validation Loss: 0.6858\n",
      "Epoch 181 - Training Loss: 0.6503, Validation Loss: 0.6888\n",
      "Epoch 182 - Training Loss: 0.6728, Validation Loss: 0.6835\n",
      "Epoch 183 - Training Loss: 0.7039, Validation Loss: 0.6849\n",
      "Epoch 184 - Training Loss: 0.6883, Validation Loss: 0.6836\n",
      "Epoch 185 - Training Loss: 0.7068, Validation Loss: 0.6756\n",
      "Epoch 186 - Training Loss: 0.6806, Validation Loss: 0.6796\n",
      "Epoch 187 - Training Loss: 0.6621, Validation Loss: 0.6769\n",
      "Epoch 188 - Training Loss: 0.6896, Validation Loss: 0.6729\n",
      "Epoch 189 - Training Loss: 0.6791, Validation Loss: 0.6809\n",
      "Epoch 190 - Training Loss: 0.6896, Validation Loss: 0.6796\n",
      "Epoch 191 - Training Loss: 0.6883, Validation Loss: 0.6836\n",
      "Epoch 192 - Training Loss: 0.7068, Validation Loss: 0.6849\n",
      "Epoch 193 - Training Loss: 0.6896, Validation Loss: 0.6796\n",
      "Epoch 194 - Training Loss: 0.7078, Validation Loss: 0.6875\n",
      "Epoch 195 - Training Loss: 0.6870, Validation Loss: 0.6769\n",
      "Epoch 196 - Training Loss: 0.7169, Validation Loss: 0.6822\n",
      "Epoch 197 - Training Loss: 0.7091, Validation Loss: 0.6836\n",
      "Epoch 198 - Training Loss: 0.6895, Validation Loss: 0.6836\n",
      "Epoch 199 - Training Loss: 0.6791, Validation Loss: 0.6809\n",
      "Epoch 200 - Training Loss: 0.6882, Validation Loss: 0.6822\n",
      "Epoch 201 - Training Loss: 0.6869, Validation Loss: 0.6743\n",
      "Epoch 202 - Training Loss: 0.6972, Validation Loss: 0.6743\n",
      "Epoch 203 - Training Loss: 0.6987, Validation Loss: 0.6875\n",
      "Epoch 204 - Training Loss: 0.6674, Validation Loss: 0.6756\n",
      "Epoch 205 - Training Loss: 0.6856, Validation Loss: 0.6796\n",
      "Epoch 206 - Training Loss: 0.6783, Validation Loss: 0.6769\n",
      "Epoch 207 - Training Loss: 0.6701, Validation Loss: 0.6902\n",
      "Epoch 208 - Training Loss: 0.6766, Validation Loss: 0.6836\n",
      "Epoch 209 - Training Loss: 0.7075, Validation Loss: 0.6809\n",
      "Epoch 210 - Training Loss: 0.6948, Validation Loss: 0.6836\n",
      "Epoch 211 - Training Loss: 0.6869, Validation Loss: 0.6789\n",
      "Epoch 212 - Training Loss: 0.7139, Validation Loss: 0.6787\n",
      "Epoch 213 - Training Loss: 0.6836, Validation Loss: 0.6834\n",
      "Epoch 214 - Training Loss: 0.6796, Validation Loss: 0.6835\n",
      "Epoch 215 - Training Loss: 0.6587, Validation Loss: 0.6928\n",
      "Epoch 216 - Training Loss: 0.6661, Validation Loss: 0.6861\n",
      "Epoch 217 - Training Loss: 0.6606, Validation Loss: 0.6821\n",
      "Epoch 218 - Training Loss: 0.6792, Validation Loss: 0.6888\n",
      "Epoch 219 - Training Loss: 0.6701, Validation Loss: 0.6888\n",
      "Epoch 220 - Training Loss: 0.6414, Validation Loss: 0.6797\n",
      "Epoch 221 - Training Loss: 0.6712, Validation Loss: 0.6770\n",
      "Epoch 222 - Training Loss: 0.6691, Validation Loss: 0.6782\n",
      "Epoch 223 - Training Loss: 0.7806, Validation Loss: 0.9260\n",
      "Epoch 224 - Training Loss: 0.8687, Validation Loss: 0.9340\n",
      "Epoch 225 - Training Loss: 0.7518, Validation Loss: 0.6744\n",
      "Epoch 226 - Training Loss: 0.6830, Validation Loss: 0.6821\n",
      "Epoch 227 - Training Loss: 0.6622, Validation Loss: 0.6848\n",
      "Epoch 228 - Training Loss: 0.6832, Validation Loss: 0.6901\n",
      "Epoch 229 - Training Loss: 0.6740, Validation Loss: 0.6914\n",
      "Epoch 230 - Training Loss: 0.6845, Validation Loss: 0.6927\n",
      "Epoch 231 - Training Loss: 0.6752, Validation Loss: 0.6873\n",
      "Epoch 232 - Training Loss: 0.6780, Validation Loss: 0.6943\n",
      "Epoch 233 - Training Loss: 0.6642, Validation Loss: 0.6874\n",
      "Epoch 234 - Training Loss: 0.6838, Validation Loss: 0.6861\n",
      "Epoch 235 - Training Loss: 0.6650, Validation Loss: 0.6967\n",
      "Epoch 236 - Training Loss: 0.6747, Validation Loss: 0.6744\n",
      "Epoch 237 - Training Loss: 0.6767, Validation Loss: 0.5505\n",
      "Epoch 238 - Training Loss: 0.7090, Validation Loss: 0.6928\n",
      "Epoch 239 - Training Loss: 0.6844, Validation Loss: 0.6818\n",
      "Epoch 240 - Training Loss: 0.6947, Validation Loss: 0.6862\n",
      "Epoch 241 - Training Loss: 0.6974, Validation Loss: 0.6822\n",
      "Epoch 242 - Training Loss: 0.6866, Validation Loss: 0.6809\n",
      "Epoch 243 - Training Loss: 0.6778, Validation Loss: 0.6782\n",
      "Epoch 244 - Training Loss: 0.7237, Validation Loss: 0.6836\n",
      "Epoch 245 - Training Loss: 0.6870, Validation Loss: 0.6809\n",
      "Epoch 246 - Training Loss: 0.6886, Validation Loss: 0.6822\n",
      "Epoch 247 - Training Loss: 0.6775, Validation Loss: 0.6862\n",
      "Epoch 248 - Training Loss: 0.6957, Validation Loss: 0.6806\n",
      "Epoch 249 - Training Loss: 0.6766, Validation Loss: 0.6784\n",
      "Epoch 250 - Training Loss: 0.6661, Validation Loss: 0.6850\n",
      "Epoch 251 - Training Loss: 0.7065, Validation Loss: 0.6797\n",
      "Epoch 252 - Training Loss: 0.6777, Validation Loss: 0.6797\n",
      "Epoch 253 - Training Loss: 0.6692, Validation Loss: 0.6798\n",
      "Epoch 254 - Training Loss: 0.6724, Validation Loss: 0.6954\n",
      "Epoch 255 - Training Loss: 0.6726, Validation Loss: 0.6914\n",
      "Epoch 256 - Training Loss: 0.6813, Validation Loss: 0.6888\n",
      "Epoch 257 - Training Loss: 0.6640, Validation Loss: 0.6861\n",
      "Epoch 258 - Training Loss: 0.6816, Validation Loss: 0.6835\n",
      "Epoch 259 - Training Loss: 0.6897, Validation Loss: 0.6874\n",
      "Epoch 260 - Training Loss: 0.6935, Validation Loss: 0.6788\n",
      "Epoch 261 - Training Loss: 0.6911, Validation Loss: 0.6730\n",
      "Epoch 262 - Training Loss: 0.6746, Validation Loss: 0.6823\n",
      "Epoch 263 - Training Loss: 0.6694, Validation Loss: 0.6747\n",
      "Epoch 264 - Training Loss: 0.6635, Validation Loss: 0.6782\n",
      "Epoch 265 - Training Loss: 0.6622, Validation Loss: 0.6821\n",
      "Epoch 266 - Training Loss: 0.6870, Validation Loss: 0.6901\n",
      "Epoch 267 - Training Loss: 0.6649, Validation Loss: 0.6861\n",
      "Epoch 268 - Training Loss: 0.6847, Validation Loss: 0.6874\n",
      "Epoch 269 - Training Loss: 0.6828, Validation Loss: 0.6821\n",
      "Epoch 270 - Training Loss: 0.6570, Validation Loss: 0.6821\n",
      "Epoch 271 - Training Loss: 0.6626, Validation Loss: 0.6888\n",
      "Epoch 272 - Training Loss: 0.6589, Validation Loss: 0.6845\n",
      "Epoch 273 - Training Loss: 0.6726, Validation Loss: 0.6864\n",
      "Epoch 274 - Training Loss: 0.6778, Validation Loss: 0.6808\n",
      "Epoch 275 - Training Loss: 0.6752, Validation Loss: 0.6833\n",
      "Epoch 276 - Training Loss: 0.7052, Validation Loss: 0.6779\n",
      "Epoch 277 - Training Loss: 0.6674, Validation Loss: 0.6845\n",
      "Epoch 278 - Training Loss: 0.6857, Validation Loss: 0.6832\n",
      "Epoch 279 - Training Loss: 0.6857, Validation Loss: 0.6832\n",
      "Epoch 280 - Training Loss: 0.6957, Validation Loss: 0.6784\n",
      "Epoch 281 - Training Loss: 0.6661, Validation Loss: 0.6848\n",
      "Epoch 282 - Training Loss: 0.6935, Validation Loss: 0.6914\n",
      "Epoch 283 - Training Loss: 0.6895, Validation Loss: 0.6835\n",
      "Epoch 284 - Training Loss: 0.6975, Validation Loss: 0.6848\n",
      "Epoch 285 - Training Loss: 0.6843, Validation Loss: 0.6835\n",
      "Epoch 286 - Training Loss: 0.6765, Validation Loss: 0.6901\n",
      "Epoch 287 - Training Loss: 0.6700, Validation Loss: 0.6795\n",
      "Epoch 288 - Training Loss: 0.6792, Validation Loss: 0.6901\n",
      "Epoch 289 - Training Loss: 0.6973, Validation Loss: 0.6928\n",
      "Epoch 290 - Training Loss: 0.6785, Validation Loss: 0.6928\n",
      "Epoch 291 - Training Loss: 0.6752, Validation Loss: 0.6874\n",
      "Epoch 292 - Training Loss: 0.6870, Validation Loss: 0.6755\n",
      "Epoch 293 - Training Loss: 0.6873, Validation Loss: 0.6874\n",
      "Epoch 294 - Training Loss: 0.6665, Validation Loss: 0.6861\n",
      "Epoch 295 - Training Loss: 0.6781, Validation Loss: 0.6874\n",
      "Epoch 296 - Training Loss: 0.6857, Validation Loss: 0.6808\n",
      "Epoch 297 - Training Loss: 0.6778, Validation Loss: 0.6888\n",
      "Epoch 298 - Training Loss: 0.7077, Validation Loss: 0.6901\n",
      "Epoch 299 - Training Loss: 0.6792, Validation Loss: 0.6874\n",
      "Epoch 300 - Training Loss: 0.6622, Validation Loss: 0.6901\n",
      "Test predictions saved to C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\test_predictions_ELU_testing_almost_final.csv\n",
      "Test predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>Training Loss</td><td>▅▄▄▇▃▅▃▃▄▄▃▅▃▇▄▅▅▇▄▃▅▂▆▃▃▅▅▃▁▂▃▃█▄▅▅▂▁▄▃</td></tr><tr><td>Validation Loss</td><td>▁▁▁▁▁▁▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>300</td></tr><tr><td>Training Loss</td><td>0.66222</td></tr><tr><td>Validation Loss</td><td>0.6901</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">quiet-meadow-40</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/3556bhjg' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/3556bhjg</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_174645-3556bhjg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # argparse.Namespace로 인자를 직접 설정합니다.\n",
    "    args = argparse.Namespace(\n",
    "        train_csv_path=\"train.csv\",                # 실제 훈련 데이터 경로\n",
    "        test_csv_path=\"test.csv\",                  # 실제 테스트 데이터 경로\n",
    "        output_path=\"test_predictions_ELU_testing_almost_final.csv\",    # 테스트 예측 결과 파일 경로\n",
    "        wandb=True,                                # WandB 사용 여부 (True로 설정하면 WandB 사용)\n",
    "        batch_size=64,                             # 배치 사이즈\n",
    "        epochs=300,                                # 학습 에폭 수\n",
    "        learning_rate=0.001,                       # 학습률\n",
    "        hidden_units=[128, 64]                     # 은닉층 크기 리스트\n",
    "    )\n",
    "\n",
    "    # main 함수 호출\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa175b-b114-4bf7-b850-dc3af2c315e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 에러 발생\n",
    "\n",
    "발견 - 하루 10개 이상 제출을 못함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6356477-f8df-46f7-9930-87510dc78365",
   "metadata": {},
   "source": [
    "![competition78](./Per_day_allow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d05f2e-d1d1-412e-9a80-6690221b53df",
   "metadata": {},
   "source": [
    "# 요구사항2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52699522-654d-4f07-b5e8-d5f566aab087",
   "metadata": {},
   "source": [
    "기존 코드에서 수정하여 특정 데이터셋에 맞게 고치기\n",
    "\n",
    "하단 코드의 경우 .py파일 기준으로 작성됨\n",
    "\n",
    "python verseion : 3.1x\\\n",
    "os : windows11\\\n",
    "개발환경 : pycharm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865bd35-d735-4e37-8c44-b3215f86f66e",
   "metadata": {},
   "source": [
    "![Rellu](./Rellu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8eb40-875d-4ea4-a00d-59afe0f6754d",
   "metadata": {},
   "source": [
    "![PREU](./PREU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e229cc2-7b06-4c05-b451-f1b9cfd200ce",
   "metadata": {},
   "source": [
    "![liki_Relu](./liki_Relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a5083-24f4-4376-89ff-08b1ff65bdb4",
   "metadata": {},
   "source": [
    "![ELU](./ELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f11be7-aa51-47c5-8d0b-981b9ec5bc48",
   "metadata": {},
   "source": [
    "![ReLU](./ReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c409c2-589a-468b-9013-47275a487e55",
   "metadata": {},
   "source": [
    "![validationLoss](./validationLoss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53901c0-02ef-4b9a-80b8-47ef0c8865f1",
   "metadata": {},
   "source": [
    "address : https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c52e2-e4fd-4506-aae9-30a1bd42e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# BASE_PATH 설정 (필요에 따라 수정)\n",
    "BASE_PATH = str(Path(__file__).resolve().parent.parent.parent)  # 예: /Users/yhhan/git/link_dl\n",
    "print(BASE_PATH, \"!!!!!\")\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, csv_file, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        if is_test:\n",
    "            self.passenger_ids = self.data['PassengerId']\n",
    "            # 테스트 데이터셋에서는 'Survived' 열이 없으므로 제거하지 않습니다.\n",
    "            columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\n",
    "            self.data = self.data.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "        else:\n",
    "            # 훈련 데이터셋에서는 'Survived' 열이 필요하므로 제거하지 않습니다.\n",
    "            columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\n",
    "            self.data = self.data.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "        # 'Sex' 인코딩: male=0, female=1\n",
    "        self.data['Sex'] = self.data['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "        # 'Age' 결측값을 평균으로 채움\n",
    "        self.data['Age'] = self.data['Age'].fillna(self.data['Age'].mean())\n",
    "\n",
    "        if not is_test:\n",
    "            # 입력 특징과 타겟 분리\n",
    "            self.X = self.data[['Pclass', 'Sex', 'Age']].values.astype(float)\n",
    "            self.y = self.data['Survived'].values.astype(float)\n",
    "        else:\n",
    "            # 테스트 데이터셋에서는 타겟이 없음\n",
    "            self.X = self.data[['Pclass', 'Sex', 'Age']].values.astype(float)\n",
    "            self.y = None\n",
    "\n",
    "        # 정규화\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X = self.scaler.fit_transform(self.X)\n",
    "\n",
    "        # 텐서로 변환\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "        if not is_test:\n",
    "            self.y = torch.tensor(self.y, dtype=torch.float32).unsqueeze(1)  # (N, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            # 테스트 데이터셋에서는 PassengerId와 함께 반환\n",
    "            return self.X[idx], self.passenger_ids.iloc[idx]\n",
    "\n",
    "def get_data(csv_path):\n",
    "    dataset = TitanicDataset(csv_file=csv_path, is_test=False)\n",
    "    print(dataset)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "    print(f\"Train size: {len(train_dataset)}, Validation size: {len(validation_dataset)}\")\n",
    "\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "def get_test_data(csv_path):\n",
    "    test_dataset = TitanicDataset(csv_file=csv_path, is_test=True)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "    return test_data_loader\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.ELU(),  # ReLU에서 ELU로 변경\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ELU(),  # ReLU에서 ELU로 변경\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "            nn.Sigmoid()  # 이진 분류를 위해 Sigmoid 활성화 추가\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "    my_model = MyModel(n_input=3, n_output=1)  # 입력 특징 수에 맞게 조정\n",
    "    optimizer = optim.Adam(my_model.parameters(), lr=wandb.config.learning_rate)  # Adam 옵티마이저 사용\n",
    "\n",
    "    return my_model, optimizer\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.BCELoss()  # 이진 분류를 위한 손실 함수\n",
    "    next_print_epoch = 100\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        all_preds_train = []\n",
    "        all_targets_train = []\n",
    "\n",
    "        for train_batch in train_data_loader:\n",
    "            input, target = train_batch\n",
    "            output_train = model(input)\n",
    "            loss = loss_fn(output_train, target)\n",
    "            loss_train += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = (output_train > 0.5).float()\n",
    "            all_preds_train.extend(preds.cpu().numpy())\n",
    "            all_targets_train.extend(target.cpu().numpy())\n",
    "\n",
    "        train_loss = loss_train / len(train_data_loader)\n",
    "        train_accuracy = accuracy_score(all_targets_train, all_preds_train)\n",
    "        train_precision = precision_score(all_targets_train, all_preds_train)\n",
    "        train_recall = recall_score(all_targets_train, all_preds_train)\n",
    "        train_f1 = f1_score(all_targets_train, all_preds_train)\n",
    "\n",
    "        model.eval()\n",
    "        loss_validation = 0.0\n",
    "        all_preds_val = []\n",
    "        all_targets_val = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for validation_batch in validation_data_loader:\n",
    "                input, target = validation_batch\n",
    "                output_validation = model(input)\n",
    "                loss = loss_fn(output_validation, target)\n",
    "                loss_validation += loss.item()\n",
    "\n",
    "                preds = (output_validation > 0.5).float()\n",
    "                all_preds_val.extend(preds.cpu().numpy())\n",
    "                all_targets_val.extend(target.cpu().numpy())\n",
    "\n",
    "        val_loss = loss_validation / len(validation_data_loader)\n",
    "        val_accuracy = accuracy_score(all_targets_val, all_preds_val)\n",
    "        val_precision = precision_score(all_targets_val, all_preds_val)\n",
    "        val_recall = recall_score(all_targets_val, all_preds_val)\n",
    "        val_f1 = f1_score(all_targets_val, all_preds_val)\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training Loss\": train_loss,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Training Accuracy\": train_accuracy,\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Training Precision\": train_precision,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Training Recall\": train_recall,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Training F1 Score\": train_f1,\n",
    "            \"Validation F1 Score\": val_f1\n",
    "        })\n",
    "\n",
    "        if epoch >= next_print_epoch:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, \"\n",
    "                f\"Training Loss: {train_loss:.4f}, \"\n",
    "                f\"Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Acc: {train_accuracy:.4f}, \"\n",
    "                f\"Validation Acc: {val_accuracy:.4f}, \"\n",
    "                f\"Training Precision: {train_precision:.4f}, \"\n",
    "                f\"Validation Precision: {val_precision:.4f}, \"\n",
    "                f\"Training Recall: {train_recall:.4f}, \"\n",
    "                f\"Validation Recall: {val_recall:.4f}, \"\n",
    "                f\"Training F1: {train_f1:.4f}, \"\n",
    "                f\"Validation F1: {val_f1:.4f}\"\n",
    "            )\n",
    "            next_print_epoch += 100\n",
    "\n",
    "def predict_test(model, test_data_loader, output_path):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_passenger_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_data_loader:\n",
    "            input, passenger_ids = test_batch\n",
    "            output = model(input)\n",
    "            preds = (output > 0.5).int()\n",
    "            # preds를 1차원으로 변환하여 리스트에 추가\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_passenger_ids.extend(passenger_ids.cpu().numpy())\n",
    "\n",
    "    # 결과를 CSV 파일로 저장\n",
    "    preds_df = pd.DataFrame({\n",
    "        'PassengerId': all_passenger_ids,\n",
    "        'Survived': all_preds\n",
    "    })\n",
    "    preds_df.to_csv(output_path, index=False)\n",
    "    print(f\"Test predictions saved to {output_path}\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'n_hidden_unit_list': args.hidden_units,  # 리스트로 받도록 수정\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"titanic_model_training\",\n",
    "        notes=\"Titanic survival prediction experiment\",\n",
    "        tags=[\"titanic\", \"binary_classification\"],\n",
    "        name=current_time_str,\n",
    "        config=config\n",
    "    )\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    # 훈련 및 검증 데이터 로더\n",
    "    train_data_loader, validation_data_loader = get_data(csv_path=args.train_csv_path)\n",
    "\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "    # 훈련 루프 실행\n",
    "    training_loop(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_data_loader=train_data_loader,\n",
    "        validation_data_loader=validation_data_loader\n",
    "    )\n",
    "\n",
    "    # 테스트 데이터 로더 (타겟 없음)\n",
    "    test_data_loader = get_test_data(csv_path=args.test_csv_path)\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 수행 및 저장\n",
    "    predict_test(model, test_data_loader, output_path=args.output_path)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Enable or disable WandB logging\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-b\", \"--batch_size\", type=int, default=32, help=\"Batch size (int, default: 32)\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-e\", \"--epochs\", type=int, default=100, help=\"Number of training epochs (int, default: 100)\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=1e-3, help=\"Learning rate (float, default: 1e-3)\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units\", type=int, nargs='+', default=[64, 32], help=\"List of hidden units (default: [64, 32])\"\n",
    "    )\n",
    "\n",
    "    # 수정된 인자\n",
    "    parser.add_argument(\n",
    "        \"--train_csv_path\", type=str, required=True, help=\"Path to the training CSV dataset\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--test_csv_path\", type=str, required=True, help=\"Path to the testing CSV dataset\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output_path\", type=str, required=True, help=\"Path to save test predictions\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057ca82-a656-4f9e-a390-713d9b3369f7",
   "metadata": {},
   "source": [
    "# 요구사항3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ebc54-f883-4ee0-aebd-7b133ada04bc",
   "metadata": {},
   "source": [
    "[ELU로 선택 사유 배경]\r",
    "    1. \t전체적인 정확도와 균형있는 성능을 우선으로 선택    2. -\tRecall을 우선시한다면 ELU로 선택    3. \n",
    "-\tValidation Accuracy와 F1 Score의 경우를 봤을 때 ReLU가 더 좋은 것으로 판    4. \r\n",
    "-\tTranining Loss의 경우엔 낮을수록 좋지만, 너무 낮아지면 과적합의 우려가     5. .\r\n",
    "-\tTraining Accuracy와 Training Precision은 전체적으로 높을수록    6. 다.\r\n",
    "-\tEpoch의 경우 300으로 진행함. 400이상부터는 효과적이지 않고 오히려 성능이 떨어지는것을\n",
    " [했다.\r\n",
    "-\t명령어를 통해서 계속해 수정 과정]\n",
    "    정해나갔다.\r\n",
    "-\tpython f_my_model_training_with_argparse_wandb.py --train_csv_path train.csv --test_csv_path test.csv --output_path test_predictions_ELU.csv --wandb --batch_size 64 --epochs 200 --learning_rate 0.001 --hidden_units     128 64\r\n",
    "-\tpython f_my_model_training_with_argparse_wandb.py --train_csv_path train.csv --test_csv_path test.csv --output_path test_predictions_ELU.csv --wandb --batch_size 64 --epochs 300 --learning_rate 0.001 --hidden_unit\n",
    "[시도]\n",
    "s 128 64\r\n",
    "-\thiddenunits의 값도 수정했다. 은닉층이 깊을수록 학습을 많이 할 것으로 판단했기 때문이다. 하지만 값을 많이할수록 성능이 좋다라고 말할 수가 없다고 알고있어서 -> 효과를 보진 못했다\n",
    "[코드 생성 및 submission.csv 생성] -- --output_path의 부분을 submission.csv로 변경하면 됨(.py 기준, 쥬파터의 경우엔 코드내부에서 변경) 256으로 진행.\r\n",
    "-\tpython f_my_model_training_with_argparse_wandb.py --train_csv_path train.csv --test_csv_path test.csv --output_path test_predictions_ELU.csv --wandb --batch_size 64 --epochs 200 --learning_rate 0.001 --hidden_units 256 64\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7002a99-d38f-4cf0-b5ef-5fbb3567e1de",
   "metadata": {},
   "source": [
    "# 요구사항4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df91a8-07e4-4df5-9bdd-383af7920d7a",
   "metadata": {},
   "source": [
    "![leaderboard_ranking](./leaderboard_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67297f1-2922-4890-ae7a-4e806c0c7797",
   "metadata": {},
   "source": [
    "## 발견\n",
    "\n",
    "- Leaderboard의 경우 마지막 제출을 기준으로 판단하기도 함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4567c-56e7-4242-a2e4-48ea4f36f8fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "![competition78](./78Scores_leaderboard2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f46031-d78a-4cef-9d14-9838dd77341b",
   "metadata": {},
   "source": [
    "# 소감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74d3bd-f5b8-489c-b186-0326ad379a6f",
   "metadata": {},
   "source": [
    "많은 시간을 투자하진 못했지만 그래도 짧은 시간에 최대한 시도해보려고 했던게 epoch를 200~400정도까지 바꿔가며 추세를 판단했습니다.\n",
    "\n",
    "batch 수는 한번만 하긴했지만 예를 들어 32라든가, learning rate도 0.0005로도 했던 것 같은데 시간이 오래걸리는만큼 세밀하게 학습할거라는 기대를 했지만 왜인지는 몰라도 그렇게 분석 정확도가 높지가 않던데 어떤식으로 해야 올라갈까를 고민하게된 계기가 되었으며 여러가지로 어떻게하면 올릴 수 있을까와. 어떤것을 올리면 어떤것도 영향을 끼칠까도 고민을 하고 시도하고 싶은 계기가 되었습니다. \n",
    "\n",
    "여러가지로 본인이 너무 모른다는 생각을 하게 되었지만 유익한 경험이었습니다.\n",
    "\n",
    "감사합니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
