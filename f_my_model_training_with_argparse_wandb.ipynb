{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T04:38:10.665899Z",
     "start_time": "2024-10-25T04:38:07.600423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl !!!!!\n"
     ]
    }
   ],
   "source": [
    "import torch  # 'torch' 라이브러리 import 추가\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 하단의 경우엔 .py파일에서 진행\n",
    "# BASE_PATH 설정 (필요에 따라 수정)\n",
    "#BASE_PATH = str(Path(__file__).resolve().parent.parent.parent)\n",
    "#print(BASE_PATH, \"!!!!!\")\n",
    "#sys.path.append(BASE_PATH)\n",
    "\n",
    "# 하단의 경우엔 .ipynb 파일에서 진행\n",
    "# 상단 코드의 경우엔 jupyter에서 인식이 안됨\n",
    "# Jupyter Notebook에서 현재 작업 디렉토리 설정\n",
    "BASE_PATH = str(Path(os.getcwd()).resolve().parent.parent)  # 현재 경로의 상위 두 단계를 기본 경로로 설정\n",
    "print(BASE_PATH, \"!!!!!\")\n",
    "sys.path.append(BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d018f-369f-41cb-827e-dff72a7afed9",
   "metadata": {},
   "source": [
    "Target\tFeature\n",
    "\r",
    "     • Survived:\t\t\t사망,\t\t\t생존– Feature    \n",
    " • PassengerId: 승객 \n",
    "\n",
    "Feature번    호\r\n",
    "• Name:     이름\r\n",
    "• Pclass:\t티켓\n",
    "        스\r\n",
    "Ø 1\t=\t1등석,\t2\t=\t2등석,\t3\t=    \t3등석\r\n",
    "• S\n",
    "        \t성별\r\n",
    "Ø male\t=\t남성,\tfemal    e\t=\t여성\r\n",
    "•     Age:\t나이\r\n",
    "• SibSp:\t동승한\t자매\t    /\t배우자의\t수\r\n",
    "• Parch:\t동승한\t부    모\t/\t자식의\t수\r\n",
    "• Tic    ket:\t티켓\t번호\r\n",
    "•     Fare:\t승객\t요금\r\n",
    "•     Cabin:\t방\t호수\r\n",
    "•\n",
    "        ked:\t탑승지\r\n",
    "4\r\n",
    " Ø C\t=\t셰르부르,\tQ\t=\t퀸즈타운,\tS\t=\t사우스햄프턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731493ea-2c94-484e-a957-0c7952079ea9",
   "metadata": {},
   "source": [
    "# 데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c059ddee271dea0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T04:38:14.072835Z",
     "start_time": "2024-10-25T04:38:13.339111Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, csv_file, is_test=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        if is_test:\n",
    "            self.passenger_ids = self.data['PassengerId']\n",
    "            # 테스트 데이터셋에서는 'Survived' 열이 없으므로 제거하지 않습니다.\n",
    "            columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\n",
    "            self.data = self.data.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "        else:\n",
    "            # 훈련 데이터셋에서는 'Survived' 열이 필요하므로 제거하지 않습니다.\n",
    "            columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\n",
    "            self.data = self.data.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "        # 'Sex' 인코딩: male=0, female=1\n",
    "        self.data['Sex'] = self.data['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "        # 'Age' 결측값을 평균으로 채움\n",
    "        self.data['Age'] = self.data['Age'].fillna(self.data['Age'].mean())\n",
    "\n",
    "        if not is_test:\n",
    "            # 입력 특징과 타겟 분리\n",
    "            self.X = self.data[['Pclass', 'Sex', 'Age']].values.astype(float)\n",
    "            self.y = self.data['Survived'].values.astype(float)\n",
    "        else:\n",
    "            # 테스트 데이터셋에서는 타겟이 없음\n",
    "            self.X = self.data[['Pclass', 'Sex', 'Age']].values.astype(float)\n",
    "            self.y = None\n",
    "\n",
    "        # 정규화\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X = self.scaler.fit_transform(self.X)\n",
    "\n",
    "        # 텐서로 변환\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "        if not is_test:\n",
    "            self.y = torch.tensor(self.y, dtype=torch.float32).unsqueeze(1)  # (N, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            # 테스트 데이터셋에서는 PassengerId와 함께 반환\n",
    "            return self.X[idx], self.passenger_ids.iloc[idx]\n",
    "\n",
    "def get_data(csv_path):\n",
    "    dataset = TitanicDataset(csv_file=csv_path, is_test=False)\n",
    "    print(dataset)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "    print(f\"Train size: {len(train_dataset)}, Validation size: {len(validation_dataset)}\")\n",
    "\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "def get_test_data(csv_path):\n",
    "    test_dataset = TitanicDataset(csv_file=csv_path, is_test=True)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "    return test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94660580-e1a1-4fea-94be-b1cdaf4a7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.ELU(),  # ReLU에서 ELU로 변경\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ELU(),  # ReLU에서 ELU로 변경\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "            nn.Sigmoid()  # 이진 분류를 위해 Sigmoid 활성화 추가\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f14ce5fa-ca1f-470e-b1c6-528851e41759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_optimizer():\n",
    "    my_model = MyModel(n_input=3, n_output=1)  # 입력 특징 수에 맞게 조정\n",
    "    optimizer = optim.Adam(my_model.parameters(), lr=wandb.config.learning_rate)  # Adam 옵티마이저 사용\n",
    "\n",
    "    return my_model, optimizer\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.BCELoss()  # 이진 분류를 위한 손실 함수\n",
    "    next_print_epoch = 100\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        all_preds_train = []\n",
    "        all_targets_train = []\n",
    "\n",
    "        for train_batch in train_data_loader:\n",
    "            input, target = train_batch\n",
    "            output_train = model(input)\n",
    "            loss = loss_fn(output_train, target)\n",
    "            loss_train += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = (output_train > 0.5).float()\n",
    "            all_preds_train.extend(preds.cpu().numpy())\n",
    "            all_targets_train.extend(target.cpu().numpy())\n",
    "\n",
    "        train_loss = loss_train / len(train_data_loader)\n",
    "        train_accuracy = accuracy_score(all_targets_train, all_preds_train)\n",
    "        train_precision = precision_score(all_targets_train, all_preds_train)\n",
    "        train_recall = recall_score(all_targets_train, all_preds_train)\n",
    "        train_f1 = f1_score(all_targets_train, all_preds_train)\n",
    "\n",
    "        model.eval()\n",
    "        loss_validation = 0.0\n",
    "        all_preds_val = []\n",
    "        all_targets_val = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for validation_batch in validation_data_loader:\n",
    "                input, target = validation_batch\n",
    "                output_validation = model(input)\n",
    "                loss = loss_fn(output_validation, target)\n",
    "                loss_validation += loss.item()\n",
    "\n",
    "                preds = (output_validation > 0.5).float()\n",
    "                all_preds_val.extend(preds.cpu().numpy())\n",
    "                all_targets_val.extend(target.cpu().numpy())\n",
    "\n",
    "        val_loss = loss_validation / len(validation_data_loader)\n",
    "        val_accuracy = accuracy_score(all_targets_val, all_preds_val)\n",
    "        val_precision = precision_score(all_targets_val, all_preds_val)\n",
    "        val_recall = recall_score(all_targets_val, all_preds_val)\n",
    "        val_f1 = f1_score(all_targets_val, all_preds_val)\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training Loss\": train_loss,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Training Accuracy\": train_accuracy,\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Training Precision\": train_precision,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Training Recall\": train_recall,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Training F1 Score\": train_f1,\n",
    "            \"Validation F1 Score\": val_f1\n",
    "        })\n",
    "\n",
    "        if epoch >= next_print_epoch:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, \"\n",
    "                f\"Training Loss: {train_loss:.4f}, \"\n",
    "                f\"Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Acc: {train_accuracy:.4f}, \"\n",
    "                f\"Validation Acc: {val_accuracy:.4f}, \"\n",
    "                f\"Training Precision: {train_precision:.4f}, \"\n",
    "                f\"Validation Precision: {val_precision:.4f}, \"\n",
    "                f\"Training Recall: {train_recall:.4f}, \"\n",
    "                f\"Validation Recall: {val_recall:.4f}, \"\n",
    "                f\"Training F1: {train_f1:.4f}, \"\n",
    "                f\"Validation F1: {val_f1:.4f}\"\n",
    "            )\n",
    "            next_print_epoch += 100\n",
    "\n",
    "def predict_test(model, test_data_loader, output_path):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_passenger_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_data_loader:\n",
    "            input, passenger_ids = test_batch\n",
    "            output = model(input)\n",
    "            preds = (output > 0.5).int()\n",
    "            # preds를 1차원으로 변환하여 리스트에 추가\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_passenger_ids.extend(passenger_ids.cpu().numpy())\n",
    "\n",
    "    # 결과를 CSV 파일로 저장\n",
    "    preds_df = pd.DataFrame({\n",
    "        'PassengerId': all_passenger_ids,\n",
    "        'Survived': all_preds\n",
    "    })\n",
    "    preds_df.to_csv(output_path, index=False)\n",
    "    print(f\"Test predictions saved to {output_path}\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'n_hidden_unit_list': args.hidden_units,  # 리스트로 받도록 수정\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"titanic_model_training\",\n",
    "        notes=\"Titanic survival prediction experiment\",\n",
    "        tags=[\"titanic\", \"binary_classification\"],\n",
    "        name=current_time_str,\n",
    "        config=config\n",
    "    )\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    # 훈련 및 검증 데이터 로더\n",
    "    train_data_loader, validation_data_loader = get_data(csv_path=args.train_csv_path)\n",
    "\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "    # 훈련 루프 실행\n",
    "    training_loop(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_data_loader=train_data_loader,\n",
    "        validation_data_loader=validation_data_loader\n",
    "    )\n",
    "\n",
    "    # 테스트 데이터 로더 (타겟 없음)\n",
    "    test_data_loader = get_test_data(csv_path=args.test_csv_path)\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 수행 및 저장\n",
    "    predict_test(model, test_data_loader, output_path=args.output_path)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02027c70-8492-4651-873d-d89b177e6224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hp6m9v4f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-25_14-16-04</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/hp6m9v4f' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/hp6m9v4f</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_141607-hp6m9v4f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hp6m9v4f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\wandb\\run-20241025_141715-5wjppshl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/5wjppshl' target=\"_blank\">2024-10-25_14-17-15</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/5wjppshl' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/5wjppshl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(train_csv_path='train.csv', test_csv_path='test.csv', output_path='test_predictions_ELU_test_using_args.csv', wandb=True, batch_size=64, epochs=200, learning_rate=0.001, hidden_units=[128, 64])\n",
      "{'epochs': 200, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [128, 64]}\n",
      "<__main__.TitanicDataset object at 0x000001AE9E6A0C70>\n",
      "Train size: 712, Validation size: 179\n",
      "Epoch 100, Training Loss: 0.4818, Validation Loss: 0.4000, Training Acc: 0.7907, Validation Acc: 0.7989, Training Precision: 0.7807, Validation Precision: 0.7636, Training Recall: 0.6426, Validation Recall: 0.6462, Training F1: 0.7050, Validation F1: 0.7000\n",
      "Epoch 200, Training Loss: 0.4354, Validation Loss: 0.3927, Training Acc: 0.7978, Validation Acc: 0.7989, Training Precision: 0.8844, Validation Precision: 0.7843, Training Recall: 0.5523, Validation Recall: 0.6154, Training F1: 0.6800, Validation F1: 0.6897\n",
      "Test predictions saved to test_predictions_ELU_test_using_args.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>Training Accuracy</td><td>▃▆▅▁▃▄▄▄▃▄▅▅▁▄▄▄▆▆▄▆▅▅▇▅▅▃▇▆▃▃▂▇▆▆▄▇▆▄▅█</td></tr><tr><td>Training F1 Score</td><td>▃▆▅▆▇▁▄▃▁▃▅▁▄▅▄▅▆▇▂▄▇▆▇▇▄▃▆▃▄▇█▇▅▆▆▅▆▂▅▇</td></tr><tr><td>Training Loss</td><td>▄▅▄▇▅▃▅▇▇▆▂▃█▇▅▅▃▁▅▄▄▅▅▃▁▃▁▃▆▃▂▁▃█▄▅▅▃▄▄</td></tr><tr><td>Training Precision</td><td>▁▁▄▂▁▅▅▂▅▄▅▅▃▄▅▂▅▄▇▄▂▄▆▄▄▂█▄▅▄▆▂▅▇▅▃▄▂▂▄</td></tr><tr><td>Training Recall</td><td>███▅▇▄▄▅▂▃▁█▆▆▃▇█▅▃▂▄▇▃▄▂▇▄▄▂▃▄▄▅▂▆▄▅▆▆▇</td></tr><tr><td>Validation Accuracy</td><td>▆▆▆▁▇▄▆▆▅▆▄▃▃▆▆▅▃▃▆▃▅▄▅▃▄▅▄▅▄▃█▄▄▅▆▆▃▅▇▃</td></tr><tr><td>Validation F1 Score</td><td>▁█▅▇▅▃▄▃▇▄▄▄▄▄▃█▄▃▄▄█▂▄▅▄▄▅▄▄▇▄▄▄▄▄▃▄▃▃▄</td></tr><tr><td>Validation Loss</td><td>█▁▁▁▂▂▂▂▄▃▂▃▃▁▃▁▂▃▃▃▂▂▄▃▁▁▃▃▁▂▁▂▂▂▁▄▂▂▂▄</td></tr><tr><td>Validation Precision</td><td>▁▃▄▅▂▄▅▆█▆▇▆▄█▃▅▃▅██▃▆▅▅▆▆▄▅▇▅▄▇▄▄▃▃▅▄▄▃</td></tr><tr><td>Validation Recall</td><td>▇█▇▃▄▇▃█▄▄▄▄▃▃▄▃█▇▃▃▁▃▄█▇▄▄▇▃▄▃▃▄▄▃▅▃███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>200</td></tr><tr><td>Training Accuracy</td><td>0.79775</td></tr><tr><td>Training F1 Score</td><td>0.68</td></tr><tr><td>Training Loss</td><td>0.4354</td></tr><tr><td>Training Precision</td><td>0.88439</td></tr><tr><td>Training Recall</td><td>0.55235</td></tr><tr><td>Validation Accuracy</td><td>0.79888</td></tr><tr><td>Validation F1 Score</td><td>0.68966</td></tr><tr><td>Validation Loss</td><td>0.39271</td></tr><tr><td>Validation Precision</td><td>0.78431</td></tr><tr><td>Validation Recall</td><td>0.61538</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-25_14-17-15</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/5wjppshl' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/5wjppshl</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_141715-5wjppshl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 명령어 인자를 직접 설정하는 방식으로 수정\n",
    "    args = argparse.Namespace(\n",
    "        train_csv_path=\"train.csv\",  # 실제 경로로 수정\n",
    "        test_csv_path=\"test.csv\",    # 실제 경로로 수정\n",
    "        output_path=\"test_predictions_ELU_test_using_args.csv\",  # 실제 경로로 수정\n",
    "        wandb=True,                  # WandB 사용 여부\n",
    "        batch_size=64,               # 배치 사이즈\n",
    "        epochs=200,                  # 학습 에폭\n",
    "        learning_rate=0.001,         # 학습률\n",
    "        hidden_units=[128, 64]       # 은닉층 크기\n",
    "    )\n",
    "\n",
    "    # main 함수 호출\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f438b8-00d2-41a3-8746-887a1667a1b5",
   "metadata": {},
   "source": [
    "![competition78](./78Scores_leaderboard.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681f68e-4a3a-426b-b380-c7132bb90bb4",
   "metadata": {},
   "source": [
    "절대 경로로 진행하려고 했지만 너무 안되었고 상대경로는 되지만 절대경로는 안되는것을 미루어보아\n",
    "jupyter라는 가상 서버를 이용하기 때문에 경로를 잘못인식을 하는건가 싶다.\n",
    "왜냐하면 getcwd()를 했음에도 문제가 없었기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002ab01-f111-469f-b500-ec03c5a2e81d",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Enable or disable WandB logging\")\n",
    "    parser.add_argument(\n",
    "        \"-b\", \"--batch_size\", type=int, default=32, help=\"Batch size (int, default: 32)\")\n",
    "    parser.add_argument(\n",
    "        \"-e\", \"--epochs\", type=int, default=100, help=\"Number of training epochs (int, default: 100)\")\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=1e-3, help=\"Learning rate (float, default: 1e-3)\")\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units\", type=int, nargs='+', default=[64, 32], help=\"List of hidden units (default: [64, 32])\")\n",
    "    # 수정된 인자\n",
    "    parser.add_argument(\n",
    "        \"--train_csv_path\", type=str, required=True, help=\"Path to the training CSV dataset\")\n",
    "    parser.add_argument(\n",
    "        \"--test_csv_path\", type=str, required=True, help=\"Path to the testing CSV dataset\")\n",
    "    parser.add_argument(\n",
    "        \"--output_path\", type=str, required=True, help=\"Path to save test predictions\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\n",
    "위 코드로 진행하려고 한다면 terminal에서 python f_my_model_training_with_argparse_wandb.py --train_csv_path train.csv --test_csv_path test.csv --output_path test_predictions_ELU.csv --wandb --batch_size 64 --epochs 200 --learning_rate 0.001 --hidden_units 128 64 이렇게 작성하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebca62-bee5-4db3-9be4-928c6bf7f0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df686ca3-a398-47c3-b0be-44be8a039c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec179784-8e83-43cd-a50c-c5c55bb66293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677c1ee-a648-4b89-a9b1-7ecbc8bb82cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bde49f-5155-4d84-9e41-498e8d6f22df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa31644c-88f3-45b3-b9ab-c9f357cc2f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6850df-dd64-4146-b83e-feb88f0048d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9d61b-1e4f-4faf-8457-af2dbfc5f4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa20408-e15f-4c7c-a3a7-b5efbb76d961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969aaf7c-9e3e-4687-be42-c66a5cd448a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa7d79-af18-4ac3-a7d0-08be250b1e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc34af-490c-440d-af93-4b559678b50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8c80a-2bb7-4d5a-a794-2dfc08f1c265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c540c4-319e-4559-974e-c2bdc14c4c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e7ee8-f7de-4b1e-ad56-811152bf3035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a330ed3-b7f9-4856-8b6d-29846773ea2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a752160-7c5c-47fa-b7e0-ba77fe0c70f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190d07a-a91a-4313-b9d5-bc27033cd2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fbb33-b711-4ca2-8690-257d9d725644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafb7d4-b70b-485a-871d-f9201938841a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2e1d4-0122-47b0-87dd-5e881afacb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "아래처럼 나와야한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4606f-f350-4f06-9bfc-6e954b1a3d6f",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a50ec88-7ca0-4d0d-8a2c-e4fdbc0071b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and Dataset Class Definitions\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13edc24-9350-4bd5-b26b-425688b34a70",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f258da5-e1e4-4f04-bf4b-1e694fc8f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TitanicDataset Class for Train and Validation Data\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c04bf-0fab-4a55-a5ed-e0daebfbb48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f36a078e-4c59-4563-81ad-6448796862e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TitanicTestDataset Class for Test Data\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'input': self.X[idx]}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff35e56-e7d5-4e89-a22b-b56b699477ed",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5be0fef5-2a2d-4de0-b13c-7f57ef5666d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X, passenger_ids):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.passenger_ids = passenger_ids  # PassengerId 추가\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        passenger_id = self.passenger_ids[idx]  # PassengerId 반환\n",
    "        return {'input': feature, 'PassengerId': passenger_id}  # PassengerId와 입력 특징 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5ed53-6bae-43d7-ad79-8f492e01a38f",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "174a65ca-2409-4058-bdb1-fa2815db5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(df):\n",
    "    # 성별 인코딩 (male=0, female=1)\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_2(df):\n",
    "    # 결측값 처리 (Age의 결측값을 평균으로 채움)\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_3(df):\n",
    "    # 결측값 처리 (Fare의 결측값을 평균으로 채움)\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_4(df):\n",
    "    # 불필요한 열 제거, 단 PassengerId는 제거하지 않음\n",
    "    columns_to_drop = ['Name', 'Ticket', 'Cabin', 'Embarked']  # 'PassengerId' 제외\n",
    "    df = df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_5(df):\n",
    "    # Pclass를 범주형에서 연속형으로 처리\n",
    "    df['Pclass'] = df['Pclass'].astype(float)\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_6(df):\n",
    "    # 정규화 (필요한 경우)\n",
    "    scaler = StandardScaler()\n",
    "    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a352c-2439-47c6-a28c-eeba3b5f4dec",
   "metadata": {},
   "source": [
    "# Section 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30540c7c-2061-4851-918a-0e87ad5574a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Preprocessing\n",
    "def get_preprocessed_dataset():\n",
    "    CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    \n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    \n",
    "    # Call pre-processing functions (define as needed)\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "079fffb7-5c2f-475a-a8aa-b0bd5a2f446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Model Definition\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 30),\n",
    "            nn.ELU(),            # ReLU 대신 ELU 사용\n",
    "            nn.Linear(30, 30),\n",
    "            nn.ELU(),            # ReLU 대신 ELU 사용\n",
    "            nn.Linear(30, n_output),\n",
    "            nn.Softmax(dim=1)    # Softmax는 그대로 유지\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6a7d9-49fc-426e-b4ef-e6746eabb81f",
   "metadata": {},
   "source": [
    "# Section 3: Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e3a926d0-9ebf-4a3e-90a2-4cc95811d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Model Definition version 2\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),  # Dropout 추가\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),  # Dropout 추가\n",
    "            nn.Linear(64, n_output),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "12382bed-36a5-4b6b-8b20-cd3277307b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Training and Evaluation Loops\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        for batch in train_data_loader:\n",
    "            inputs, targets = batch['input'], batch['target']\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch} - Training loss: {loss.item()}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22865a-0480-4d84-85b6-d4d26637e0f2",
   "metadata": {},
   "source": [
    "# Section 4: Training and Evaluation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "99791aeb-63a7-492e-92c3-a8a4e9e4cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Training and Evaluation Loops_v2_wandB에 추가하기\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_data_loader:\n",
    "            inputs, targets = batch['input'], batch['target']\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation Loss 계산\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader:\n",
    "                inputs, targets = batch['input'], batch['target']\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                validation_loss += loss.item()\n",
    "        \n",
    "        validation_loss /= len(validation_data_loader)\n",
    "\n",
    "        # WandB에 기록\n",
    "        wandb.log({\"Training Loss\": train_loss, \"Validation Loss\": validation_loss, \"Epoch\": epoch})\n",
    "\n",
    "        print(f\"Epoch {epoch} - Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c5f0500-23d2-4a90-be7b-eb0c45d058c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Model Testing and Prediction - version 1\n",
    "def predict_test(model, test_data_loader,output_path=\"test_predictions_ELU_testing_almost_final.csv\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    preds_df.to_csv(output_path, index=False)\n",
    "    print(f\"Test predictions saved to {os.path.abspath(output_path)}\")  # 파일의 절대 경로 출력\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            inputs = batch['input']\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb79f96a-b7c9-4301-b41a-c04b0bdf5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Section 5: Model Testing and Prediction _version 2\n",
    "def predict_test(model, test_data_loader, output_path=\"test_predictions_ELU_testing_almost_final.csv\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            inputs = batch['input']\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "    \n",
    "    # 예측 결과를 DataFrame으로 변환\n",
    "    preds_df = pd.DataFrame({\"Predicted\": predictions})\n",
    "    \n",
    "    # DataFrame을 CSV 파일로 저장\n",
    "    preds_df.to_csv(output_path, index=False)\n",
    "    print(f\"Test predictions saved to {os.path.abspath(output_path)}\")  # 파일의 절대 경로 출력\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a10641b5-db16-4bee-93d1-9e7a07609f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Model Testing and Prediction 3차 version\n",
    "def predict_test(model, test_data_loader, output_path=\"test_predictions_ELU_testing_almost_final.csv\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    passenger_ids = []  # PassengerId 저장을 위한 리스트\n",
    "\n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            inputs = batch['input']\n",
    "            passenger_ids_batch = batch['PassengerId']  # PassengerId 가져오기\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # 예측 값과 PassengerId 저장\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            passenger_ids.extend(passenger_ids_batch.cpu().numpy())\n",
    "    \n",
    "    # 예측 결과를 DataFrame으로 변환\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"PassengerId\": passenger_ids,\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "    # DataFrame을 CSV 파일로 저장\n",
    "    preds_df.to_csv(output_path, index=False)\n",
    "    print(f\"Test predictions saved to {os.path.abspath(output_path)}\")  # 파일의 절대 경로 출력\n",
    "\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "896988fb-6256-4760-b761-dcfe289bca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4 \n",
    "import pandas as pd\n",
    "\n",
    "# Model Testing and Prediction\n",
    "def predict_test(model, test_data_loader, output_path=\"test_predictions_ELU_testing_almost_final.csv\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    passenger_ids = []  # PassengerId 저장을 위한 리스트\n",
    "\n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            inputs = batch['input']\n",
    "            passenger_ids_batch = batch['PassengerId']  # PassengerId 가져오기\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # 예측 값과 PassengerId 저장\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            passenger_ids.extend(passenger_ids_batch)\n",
    "\n",
    "    # 예측 결과를 DataFrame으로 변환\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"PassengerId\": passenger_ids,\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "    # DataFrame을 CSV 파일로 저장\n",
    "    preds_df.to_csv(output_path, index=False)\n",
    "    print(f\"Test predictions saved to {os.path.abspath(output_path)}\")  # 파일의 절대 경로 출력\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de3cb7-ff96-4dbb-9aa9-1ab4d744fba9",
   "metadata": {},
   "source": [
    "# Section 5: Model Testing and Predictio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b9d2968f-a023-4064-be89-138ee82deccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_test version 5\n",
    "# Section 5: Model Testing and Predictio\n",
    "import pandas as pd\n",
    "\n",
    "# Model Testing and Prediction\n",
    "def predict_test(model, test_data_loader, output_path=\"test_predictions_ELU_testing_almost_final.csv\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    passenger_ids = []  # PassengerId 저장을 위한 리스트\n",
    "\n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            inputs = batch['input']\n",
    "            passenger_ids_batch = batch['PassengerId']  # PassengerId 가져오기\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # 예측 값과 PassengerId 저장\n",
    "            predictions.extend(predicted_labels.cpu().numpy())  # Tensor에서 numpy로 변환\n",
    "            passenger_ids.extend(passenger_ids_batch.cpu().numpy())  # Tensor에서 numpy로 변환\n",
    "\n",
    "    # 예측 결과를 DataFrame으로 변환\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"PassengerId\": passenger_ids,\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "    # DataFrame을 CSV 파일로 저장\n",
    "    preds_df.to_csv(output_path, index=False)\n",
    "    print(f\"Test predictions saved to {os.path.abspath(output_path)}\")  # 파일의 절대 경로 출력\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbe172d0-826a-46d4-ba96-36caaa26cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "def main(args):\n",
    "    wandb.init(project=\"titanic_model_training\", config={\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"learning_rate\": args.learning_rate\n",
    "    })\n",
    "\n",
    "    # 전처리된 데이터셋 로드\n",
    "    all_df = get_preprocessed_dataset()\n",
    "    \n",
    "    # 훈련, 검증, 테스트 데이터셋 분리\n",
    "    train_df = all_df[all_df['Survived'].notna()]  # Survived 값이 있는 행을 훈련용 데이터로 사용\n",
    "    test_df = all_df[all_df['Survived'].isna()]    # Survived 값이 없는 행을 테스트 데이터로 사용\n",
    "    \n",
    "    # 특징(X)와 타겟(y) 분리\n",
    "    X_train = train_df.drop(columns=['Survived']).values\n",
    "    y_train = train_df['Survived'].values\n",
    "\n",
    "    X_test = test_df.drop(columns=['Survived']).values\n",
    "    # 테스트 데이터는 타겟(y) 값이 없으므로 y_test를 설정하지 않음\n",
    "\n",
    "    # Dataset 생성\n",
    "    full_train_dataset = TitanicDataset(X_train, y_train)\n",
    "    \n",
    "    # 훈련/검증 데이터 분리 (예: 80% 훈련, 20% 검증)\n",
    "    train_size = int(0.8 * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    test_dataset = TitanicTestDataset(X_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(validation_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model = MyModel(n_input=X_train.shape[1], n_output=2)  # 출력 크기를 2로 설정해 이진 분류 수행\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 훈련 루프 실행\n",
    "    training_loop(model, optimizer, train_data_loader, validation_data_loader)\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 수행 및 결과 출력\n",
    "    test_predictions = predict_test(model, test_data_loader)\n",
    "    print(f\"Test predictions: {test_predictions}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee4082e6-bc6f-4e1c-b2b8-8cd06421fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 main함수\n",
    "def main(args):\n",
    "    wandb.init(project=\"titanic_model_training\", config={\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"learning_rate\": args.learning_rate\n",
    "    })\n",
    "\n",
    "    # 전처리된 데이터셋 로드\n",
    "    all_df = get_preprocessed_dataset()\n",
    "    \n",
    "    # 훈련, 검증, 테스트 데이터셋 분리\n",
    "    train_df = all_df[all_df['Survived'].notna()]  # Survived 값이 있는 행을 훈련용 데이터로 사용\n",
    "    test_df = all_df[all_df['Survived'].isna()]    # Survived 값이 없는 행을 테스트 데이터로 사용\n",
    "    \n",
    "    # 특징(X)와 타겟(y) 분리\n",
    "    X_train = train_df.drop(columns=['Survived']).values\n",
    "    y_train = train_df['Survived'].values\n",
    "\n",
    "    X_test = test_df.drop(columns=['Survived']).values\n",
    "    passenger_ids_test = test_df['PassengerId'].values  # 테스트 데이터에서 PassengerId 가져오기\n",
    "\n",
    "    # Dataset 생성\n",
    "    full_train_dataset = TitanicDataset(X_train, y_train)\n",
    "    \n",
    "    # 훈련/검증 데이터 분리 (예: 80% 훈련, 20% 검증)\n",
    "    train_size = int(0.8 * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # 테스트 데이터셋에 PassengerId 포함\n",
    "    test_dataset = TitanicTestDataset(X_test, passenger_ids_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(validation_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model = MyModel(n_input=X_train.shape[1], n_output=2)  # 출력 크기를 2로 설정해 이진 분류 수행\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 훈련 루프 실행\n",
    "    training_loop(model, optimizer, train_data_loader, validation_data_loader)\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 수행 및 결과 출력\n",
    "    test_predictions = predict_test(model, test_data_loader)\n",
    "    print(f\"Test predictions: {test_predictions}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "107af542-33b1-4829-9a1c-28bc779fcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3\n",
    "def main(args):\n",
    "    wandb.init(project=\"titanic_model_training\", config={\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"learning_rate\": args.learning_rate\n",
    "    })\n",
    "\n",
    "    # 전처리된 데이터셋 로드\n",
    "    all_df = get_preprocessed_dataset()\n",
    "    \n",
    "    # 훈련, 검증, 테스트 데이터셋 분리\n",
    "    train_df = all_df[all_df['Survived'].notna()]  # Survived 값이 있는 행을 훈련용 데이터로 사용\n",
    "    test_df = all_df[all_df['Survived'].isna()]    # Survived 값이 없는 행을 테스트 데이터로 사용\n",
    "    \n",
    "    # 특징(X)와 타겟(y) 분리\n",
    "    X_train = train_df.drop(columns=['Survived']).values\n",
    "    y_train = train_df['Survived'].values\n",
    "\n",
    "    X_test = test_df.drop(columns=['Survived']).values\n",
    "    passenger_ids_test = test_df['PassengerId'].values  # 테스트 데이터에서 PassengerId 가져오기\n",
    "\n",
    "    # Dataset 생성\n",
    "    full_train_dataset = TitanicDataset(X_train, y_train)\n",
    "    \n",
    "    # 훈련/검증 데이터 분리 (예: 80% 훈련, 20% 검증)\n",
    "    train_size = int(0.8 * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # 테스트 데이터셋에 PassengerId 포함\n",
    "    test_dataset = TitanicTestDataset(X_test, passenger_ids_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(validation_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model = MyModel(n_input=X_train.shape[1], n_output=2)  # 출력 크기를 2로 설정해 이진 분류 수행\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 훈련 루프 실행\n",
    "    training_loop(model, optimizer, train_data_loader, validation_data_loader)\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 수행 및 결과 출력\n",
    "    test_predictions = predict_test(model, test_data_loader)\n",
    "    print(f\"Test predictions: {test_predictions}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "842dc36e-4caf-42b1-a061-5485ebf666b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "469e0fc9-c169-45d0-8626-37018594a419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\wandb\\run-20241025_165009-qfvy5jgw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/qfvy5jgw' target=\"_blank\">cosmic-snowflake-37</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/qfvy5jgw' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/qfvy5jgw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.7086, Validation Loss: 0.7042\n",
      "Epoch 2 - Training Loss: 0.6937, Validation Loss: 0.7016\n",
      "Epoch 3 - Training Loss: 0.7023, Validation Loss: 0.7094\n",
      "Epoch 4 - Training Loss: 0.6840, Validation Loss: 0.7077\n",
      "Epoch 5 - Training Loss: 0.6916, Validation Loss: 0.7054\n",
      "Epoch 6 - Training Loss: 0.6914, Validation Loss: 0.7046\n",
      "Epoch 7 - Training Loss: 0.7078, Validation Loss: 0.6990\n",
      "Epoch 8 - Training Loss: 0.6900, Validation Loss: 0.6987\n",
      "Epoch 9 - Training Loss: 0.6907, Validation Loss: 0.6924\n",
      "Epoch 10 - Training Loss: 0.6997, Validation Loss: 0.7028\n",
      "Epoch 11 - Training Loss: 0.6912, Validation Loss: 0.6954\n",
      "Epoch 12 - Training Loss: 0.6709, Validation Loss: 0.6956\n",
      "Epoch 13 - Training Loss: 0.7892, Validation Loss: 0.9253\n",
      "Epoch 14 - Training Loss: 0.8123, Validation Loss: 0.7048\n",
      "Epoch 15 - Training Loss: 0.7005, Validation Loss: 0.7057\n",
      "Epoch 16 - Training Loss: 0.6842, Validation Loss: 0.6991\n",
      "Epoch 17 - Training Loss: 0.7018, Validation Loss: 0.7004\n",
      "Epoch 18 - Training Loss: 0.7108, Validation Loss: 0.6991\n",
      "Epoch 19 - Training Loss: 0.6828, Validation Loss: 0.7070\n",
      "Epoch 20 - Training Loss: 0.6821, Validation Loss: 0.7070\n",
      "Epoch 21 - Training Loss: 0.6732, Validation Loss: 0.7057\n",
      "Epoch 22 - Training Loss: 0.7001, Validation Loss: 0.7030\n",
      "Epoch 23 - Training Loss: 0.6638, Validation Loss: 0.6990\n",
      "Epoch 24 - Training Loss: 0.7092, Validation Loss: 0.7110\n",
      "Epoch 25 - Training Loss: 0.7001, Validation Loss: 0.6990\n",
      "Epoch 26 - Training Loss: 0.6819, Validation Loss: 0.7083\n",
      "Epoch 27 - Training Loss: 0.7275, Validation Loss: 0.7057\n",
      "Epoch 28 - Training Loss: 0.6820, Validation Loss: 0.7123\n",
      "Epoch 29 - Training Loss: 0.6909, Validation Loss: 0.7030\n",
      "Epoch 30 - Training Loss: 0.6818, Validation Loss: 0.7043\n",
      "Epoch 31 - Training Loss: 0.6819, Validation Loss: 0.7082\n",
      "Epoch 32 - Training Loss: 0.7000, Validation Loss: 0.7069\n",
      "Epoch 33 - Training Loss: 0.6909, Validation Loss: 0.7082\n",
      "Epoch 34 - Training Loss: 0.6819, Validation Loss: 0.7015\n",
      "Epoch 35 - Training Loss: 0.6907, Validation Loss: 0.6948\n",
      "Epoch 36 - Training Loss: 0.6999, Validation Loss: 0.6999\n",
      "Epoch 37 - Training Loss: 0.7000, Validation Loss: 0.7010\n",
      "Epoch 38 - Training Loss: 0.6904, Validation Loss: 0.6988\n",
      "Epoch 39 - Training Loss: 0.6802, Validation Loss: 0.7030\n",
      "Epoch 40 - Training Loss: 0.6810, Validation Loss: 0.6941\n",
      "Epoch 41 - Training Loss: 0.6715, Validation Loss: 0.6958\n",
      "Epoch 42 - Training Loss: 0.6723, Validation Loss: 0.6901\n",
      "Epoch 43 - Training Loss: 0.6581, Validation Loss: 0.6981\n",
      "Epoch 44 - Training Loss: 0.6997, Validation Loss: 0.6890\n",
      "Epoch 45 - Training Loss: 0.6809, Validation Loss: 0.6914\n",
      "Epoch 46 - Training Loss: 0.7071, Validation Loss: 0.6940\n",
      "Epoch 47 - Training Loss: 0.6949, Validation Loss: 0.6940\n",
      "Epoch 48 - Training Loss: 0.6929, Validation Loss: 0.6943\n",
      "Epoch 49 - Training Loss: 0.6793, Validation Loss: 0.6898\n",
      "Epoch 50 - Training Loss: 0.7139, Validation Loss: 0.6955\n",
      "Epoch 51 - Training Loss: 0.6866, Validation Loss: 0.6918\n",
      "Epoch 52 - Training Loss: 0.6789, Validation Loss: 0.6937\n",
      "Epoch 53 - Training Loss: 0.6828, Validation Loss: 0.7016\n",
      "Epoch 54 - Training Loss: 0.7066, Validation Loss: 0.6864\n",
      "Epoch 55 - Training Loss: 0.7254, Validation Loss: 0.6927\n",
      "Epoch 56 - Training Loss: 0.6795, Validation Loss: 0.6940\n",
      "Epoch 57 - Training Loss: 0.6777, Validation Loss: 0.6927\n",
      "Epoch 58 - Training Loss: 0.6866, Validation Loss: 0.6927\n",
      "Epoch 59 - Training Loss: 0.6803, Validation Loss: 0.6913\n",
      "Epoch 60 - Training Loss: 0.6808, Validation Loss: 0.6953\n",
      "Epoch 61 - Training Loss: 0.7053, Validation Loss: 0.6940\n",
      "Epoch 62 - Training Loss: 0.6985, Validation Loss: 0.6874\n",
      "Epoch 63 - Training Loss: 0.6886, Validation Loss: 0.6901\n",
      "Epoch 64 - Training Loss: 0.6900, Validation Loss: 0.6914\n",
      "Epoch 65 - Training Loss: 0.6974, Validation Loss: 0.6860\n",
      "Epoch 66 - Training Loss: 0.6779, Validation Loss: 0.6940\n",
      "Epoch 67 - Training Loss: 0.6675, Validation Loss: 0.6980\n",
      "Epoch 68 - Training Loss: 0.6971, Validation Loss: 0.6887\n",
      "Epoch 69 - Training Loss: 0.6988, Validation Loss: 0.6980\n",
      "Epoch 70 - Training Loss: 0.6862, Validation Loss: 0.6967\n",
      "Epoch 71 - Training Loss: 0.6795, Validation Loss: 0.6993\n",
      "Epoch 72 - Training Loss: 0.6973, Validation Loss: 0.6914\n",
      "Epoch 73 - Training Loss: 0.6907, Validation Loss: 0.6954\n",
      "Epoch 74 - Training Loss: 0.6867, Validation Loss: 0.6993\n",
      "Epoch 75 - Training Loss: 0.6995, Validation Loss: 0.6962\n",
      "Epoch 76 - Training Loss: 0.7068, Validation Loss: 0.7124\n",
      "Epoch 77 - Training Loss: 0.7017, Validation Loss: 0.6920\n",
      "Epoch 78 - Training Loss: 0.7256, Validation Loss: 0.6978\n",
      "Epoch 79 - Training Loss: 0.6861, Validation Loss: 0.6961\n",
      "Epoch 80 - Training Loss: 0.6579, Validation Loss: 0.6950\n",
      "Epoch 81 - Training Loss: 0.6938, Validation Loss: 0.6932\n",
      "Epoch 82 - Training Loss: 0.6965, Validation Loss: 0.6960\n",
      "Epoch 83 - Training Loss: 0.6961, Validation Loss: 0.6949\n",
      "Epoch 84 - Training Loss: 0.6869, Validation Loss: 0.7008\n",
      "Epoch 85 - Training Loss: 0.6877, Validation Loss: 0.6916\n",
      "Epoch 86 - Training Loss: 0.6705, Validation Loss: 0.6983\n",
      "Epoch 87 - Training Loss: 0.7037, Validation Loss: 0.6973\n",
      "Epoch 88 - Training Loss: 0.6873, Validation Loss: 0.6942\n",
      "Epoch 89 - Training Loss: 0.6948, Validation Loss: 0.6949\n",
      "Epoch 90 - Training Loss: 0.6958, Validation Loss: 0.7045\n",
      "Epoch 91 - Training Loss: 0.6762, Validation Loss: 0.6948\n",
      "Epoch 92 - Training Loss: 0.6884, Validation Loss: 0.6974\n",
      "Epoch 93 - Training Loss: 0.6855, Validation Loss: 0.7057\n",
      "Epoch 94 - Training Loss: 0.7047, Validation Loss: 0.6951\n",
      "Epoch 95 - Training Loss: 0.6679, Validation Loss: 0.7004\n",
      "Epoch 96 - Training Loss: 0.6837, Validation Loss: 0.6911\n",
      "Epoch 97 - Training Loss: 0.6736, Validation Loss: 0.7016\n",
      "Epoch 98 - Training Loss: 0.6839, Validation Loss: 0.6925\n",
      "Epoch 99 - Training Loss: 0.7014, Validation Loss: 0.6939\n",
      "Epoch 100 - Training Loss: 0.6936, Validation Loss: 0.6951\n",
      "Epoch 101 - Training Loss: 0.6542, Validation Loss: 0.7005\n",
      "Epoch 102 - Training Loss: 0.6820, Validation Loss: 0.6955\n",
      "Epoch 103 - Training Loss: 0.7112, Validation Loss: 0.6976\n",
      "Epoch 104 - Training Loss: 0.6759, Validation Loss: 0.6976\n",
      "Epoch 105 - Training Loss: 0.6828, Validation Loss: 0.6998\n",
      "Epoch 106 - Training Loss: 0.6823, Validation Loss: 0.7004\n",
      "Epoch 107 - Training Loss: 0.6674, Validation Loss: 0.6925\n",
      "Epoch 108 - Training Loss: 0.7115, Validation Loss: 0.6964\n",
      "Epoch 109 - Training Loss: 0.7044, Validation Loss: 0.6852\n",
      "Epoch 110 - Training Loss: 0.7162, Validation Loss: 0.6993\n",
      "Epoch 111 - Training Loss: 0.6779, Validation Loss: 0.6962\n",
      "Epoch 112 - Training Loss: 0.6848, Validation Loss: 0.6913\n",
      "Epoch 113 - Training Loss: 0.6723, Validation Loss: 0.6945\n",
      "Epoch 114 - Training Loss: 0.6825, Validation Loss: 0.6923\n",
      "Epoch 115 - Training Loss: 0.6806, Validation Loss: 0.7001\n",
      "Epoch 116 - Training Loss: 0.6868, Validation Loss: 0.6972\n",
      "Epoch 117 - Training Loss: 0.6742, Validation Loss: 0.6961\n",
      "Epoch 118 - Training Loss: 0.6921, Validation Loss: 0.6916\n",
      "Epoch 119 - Training Loss: 0.6986, Validation Loss: 0.7078\n",
      "Epoch 120 - Training Loss: 0.7188, Validation Loss: 0.6925\n",
      "Epoch 121 - Training Loss: 0.6805, Validation Loss: 0.7056\n",
      "Epoch 122 - Training Loss: 0.6927, Validation Loss: 0.6967\n",
      "Epoch 123 - Training Loss: 0.6772, Validation Loss: 0.6955\n",
      "Epoch 124 - Training Loss: 0.6796, Validation Loss: 0.6941\n",
      "Epoch 125 - Training Loss: 0.6875, Validation Loss: 0.6954\n",
      "Epoch 126 - Training Loss: 0.7059, Validation Loss: 0.6913\n",
      "Epoch 127 - Training Loss: 0.6804, Validation Loss: 0.6980\n",
      "Epoch 128 - Training Loss: 0.6780, Validation Loss: 0.6980\n",
      "Epoch 129 - Training Loss: 0.6961, Validation Loss: 0.6913\n",
      "Epoch 130 - Training Loss: 0.6899, Validation Loss: 0.6940\n",
      "Epoch 131 - Training Loss: 0.6789, Validation Loss: 0.6900\n",
      "Epoch 132 - Training Loss: 0.6783, Validation Loss: 0.6953\n",
      "Epoch 133 - Training Loss: 0.6675, Validation Loss: 0.6940\n",
      "Epoch 134 - Training Loss: 0.7041, Validation Loss: 0.6953\n",
      "Epoch 135 - Training Loss: 0.6763, Validation Loss: 0.6940\n",
      "Epoch 136 - Training Loss: 0.6786, Validation Loss: 0.6913\n",
      "Epoch 137 - Training Loss: 0.7135, Validation Loss: 0.6918\n",
      "Epoch 138 - Training Loss: 0.7015, Validation Loss: 0.6999\n",
      "Epoch 139 - Training Loss: 0.6779, Validation Loss: 0.7015\n",
      "Epoch 140 - Training Loss: 0.6669, Validation Loss: 0.6965\n",
      "Epoch 141 - Training Loss: 0.6764, Validation Loss: 0.6899\n",
      "Epoch 142 - Training Loss: 0.7039, Validation Loss: 0.7005\n",
      "Epoch 143 - Training Loss: 0.7131, Validation Loss: 0.6965\n",
      "Epoch 144 - Training Loss: 0.6950, Validation Loss: 0.6992\n",
      "Epoch 145 - Training Loss: 0.6747, Validation Loss: 0.6979\n",
      "Epoch 146 - Training Loss: 0.6766, Validation Loss: 0.6952\n",
      "Epoch 147 - Training Loss: 0.6538, Validation Loss: 0.6962\n",
      "Epoch 148 - Training Loss: 0.6934, Validation Loss: 0.6986\n",
      "Epoch 149 - Training Loss: 0.6930, Validation Loss: 0.6990\n",
      "Epoch 150 - Training Loss: 0.6730, Validation Loss: 0.6929\n",
      "Epoch 151 - Training Loss: 0.6523, Validation Loss: 0.6769\n",
      "Epoch 152 - Training Loss: 0.8632, Validation Loss: 0.9184\n",
      "Epoch 153 - Training Loss: 0.9396, Validation Loss: 0.9186\n",
      "Epoch 154 - Training Loss: 0.9122, Validation Loss: 0.9277\n",
      "Epoch 155 - Training Loss: 0.9213, Validation Loss: 0.9224\n",
      "Epoch 156 - Training Loss: 0.9304, Validation Loss: 0.9183\n",
      "Epoch 157 - Training Loss: 0.9391, Validation Loss: 0.9210\n",
      "Epoch 158 - Training Loss: 0.9107, Validation Loss: 0.9159\n",
      "Epoch 159 - Training Loss: 0.9291, Validation Loss: 0.9157\n",
      "Epoch 160 - Training Loss: 0.9116, Validation Loss: 0.9129\n",
      "Epoch 161 - Training Loss: 0.9383, Validation Loss: 0.9169\n",
      "Epoch 162 - Training Loss: 0.9460, Validation Loss: 0.9182\n",
      "Epoch 163 - Training Loss: 0.9369, Validation Loss: 0.9169\n",
      "Epoch 164 - Training Loss: 0.9366, Validation Loss: 0.9196\n",
      "Epoch 165 - Training Loss: 0.9357, Validation Loss: 0.9129\n",
      "Epoch 166 - Training Loss: 0.9265, Validation Loss: 0.9143\n",
      "Epoch 167 - Training Loss: 0.9265, Validation Loss: 0.9156\n",
      "Epoch 168 - Training Loss: 0.9265, Validation Loss: 0.9196\n",
      "Epoch 169 - Training Loss: 0.9173, Validation Loss: 0.9169\n",
      "Epoch 170 - Training Loss: 0.9447, Validation Loss: 0.9143\n",
      "Epoch 171 - Training Loss: 0.9253, Validation Loss: 0.9156\n",
      "Epoch 172 - Training Loss: 0.9228, Validation Loss: 0.9209\n",
      "Epoch 173 - Training Loss: 0.9279, Validation Loss: 0.9169\n",
      "Epoch 174 - Training Loss: 0.9370, Validation Loss: 0.9129\n",
      "Epoch 175 - Training Loss: 0.9278, Validation Loss: 0.9209\n",
      "Epoch 176 - Training Loss: 0.9356, Validation Loss: 0.9143\n",
      "Epoch 177 - Training Loss: 0.9360, Validation Loss: 0.9143\n",
      "Epoch 178 - Training Loss: 0.9448, Validation Loss: 0.9182\n",
      "Epoch 179 - Training Loss: 0.9265, Validation Loss: 0.9222\n",
      "Epoch 180 - Training Loss: 0.9265, Validation Loss: 0.9156\n",
      "Epoch 181 - Training Loss: 0.9266, Validation Loss: 0.9169\n",
      "Epoch 182 - Training Loss: 0.9269, Validation Loss: 0.9156\n",
      "Epoch 183 - Training Loss: 0.9261, Validation Loss: 0.9143\n",
      "Epoch 184 - Training Loss: 0.9161, Validation Loss: 0.9129\n",
      "Epoch 185 - Training Loss: 0.9074, Validation Loss: 0.9222\n",
      "Epoch 186 - Training Loss: 0.9333, Validation Loss: 0.9116\n",
      "Epoch 187 - Training Loss: 0.9435, Validation Loss: 0.9156\n",
      "Epoch 188 - Training Loss: 0.9432, Validation Loss: 0.9196\n",
      "Epoch 189 - Training Loss: 0.9066, Validation Loss: 0.9156\n",
      "Epoch 190 - Training Loss: 0.8940, Validation Loss: 0.9116\n",
      "Epoch 191 - Training Loss: 0.9239, Validation Loss: 0.9156\n",
      "Epoch 192 - Training Loss: 0.9166, Validation Loss: 0.9209\n",
      "Epoch 193 - Training Loss: 0.9297, Validation Loss: 0.9169\n",
      "Epoch 194 - Training Loss: 0.8494, Validation Loss: 0.6872\n",
      "Epoch 195 - Training Loss: 0.7017, Validation Loss: 0.6965\n",
      "Epoch 196 - Training Loss: 0.7000, Validation Loss: 0.6979\n",
      "Epoch 197 - Training Loss: 0.6818, Validation Loss: 0.6953\n",
      "Epoch 198 - Training Loss: 0.7090, Validation Loss: 0.7020\n",
      "Epoch 199 - Training Loss: 0.6909, Validation Loss: 0.6993\n",
      "Epoch 200 - Training Loss: 0.6818, Validation Loss: 0.7020\n",
      "Epoch 201 - Training Loss: 0.7091, Validation Loss: 0.7033\n",
      "Epoch 202 - Training Loss: 0.6726, Validation Loss: 0.7033\n",
      "Epoch 203 - Training Loss: 0.7000, Validation Loss: 0.6980\n",
      "Epoch 204 - Training Loss: 0.7000, Validation Loss: 0.7006\n",
      "Epoch 205 - Training Loss: 0.6909, Validation Loss: 0.6927\n",
      "Epoch 206 - Training Loss: 0.6722, Validation Loss: 0.7032\n",
      "Epoch 207 - Training Loss: 0.6909, Validation Loss: 0.7032\n",
      "Epoch 208 - Training Loss: 0.7000, Validation Loss: 0.6952\n",
      "Epoch 209 - Training Loss: 0.6818, Validation Loss: 0.6952\n",
      "Epoch 210 - Training Loss: 0.6817, Validation Loss: 0.7019\n",
      "Epoch 211 - Training Loss: 0.6726, Validation Loss: 0.6979\n",
      "Epoch 212 - Training Loss: 0.6992, Validation Loss: 0.6992\n",
      "Epoch 213 - Training Loss: 0.6896, Validation Loss: 0.7005\n",
      "Epoch 214 - Training Loss: 0.7000, Validation Loss: 0.6979\n",
      "Epoch 215 - Training Loss: 0.7000, Validation Loss: 0.6979\n",
      "Epoch 216 - Training Loss: 0.7086, Validation Loss: 0.7005\n",
      "Epoch 217 - Training Loss: 0.6896, Validation Loss: 0.6992\n",
      "Epoch 218 - Training Loss: 0.6896, Validation Loss: 0.7045\n",
      "Epoch 219 - Training Loss: 0.6896, Validation Loss: 0.7005\n",
      "Epoch 220 - Training Loss: 0.6816, Validation Loss: 0.7005\n",
      "Epoch 221 - Training Loss: 0.6713, Validation Loss: 0.6992\n",
      "Epoch 222 - Training Loss: 0.6713, Validation Loss: 0.6979\n",
      "Epoch 223 - Training Loss: 0.6909, Validation Loss: 0.7032\n",
      "Epoch 224 - Training Loss: 0.6804, Validation Loss: 0.6939\n",
      "Epoch 225 - Training Loss: 0.6817, Validation Loss: 0.7032\n",
      "Epoch 226 - Training Loss: 0.6883, Validation Loss: 0.6979\n",
      "Epoch 227 - Training Loss: 0.6804, Validation Loss: 0.6979\n",
      "Epoch 228 - Training Loss: 0.6987, Validation Loss: 0.6952\n",
      "Epoch 229 - Training Loss: 0.6700, Validation Loss: 0.6979\n",
      "Epoch 230 - Training Loss: 0.6713, Validation Loss: 0.7058\n",
      "Epoch 231 - Training Loss: 0.6987, Validation Loss: 0.6965\n",
      "Epoch 232 - Training Loss: 0.7091, Validation Loss: 0.6979\n",
      "Epoch 233 - Training Loss: 0.7169, Validation Loss: 0.6939\n",
      "Epoch 234 - Training Loss: 0.6719, Validation Loss: 0.6965\n",
      "Epoch 235 - Training Loss: 0.7080, Validation Loss: 0.6992\n",
      "Epoch 236 - Training Loss: 0.6896, Validation Loss: 0.6992\n",
      "Epoch 237 - Training Loss: 0.6713, Validation Loss: 0.6952\n",
      "Epoch 238 - Training Loss: 0.6804, Validation Loss: 0.7045\n",
      "Epoch 239 - Training Loss: 0.7065, Validation Loss: 0.6952\n",
      "Epoch 240 - Training Loss: 0.6987, Validation Loss: 0.6965\n",
      "Epoch 241 - Training Loss: 0.6804, Validation Loss: 0.6979\n",
      "Epoch 242 - Training Loss: 0.6987, Validation Loss: 0.6965\n",
      "Epoch 243 - Training Loss: 0.6804, Validation Loss: 0.7018\n",
      "Epoch 244 - Training Loss: 0.6791, Validation Loss: 0.7018\n",
      "Epoch 245 - Training Loss: 0.6896, Validation Loss: 0.6992\n",
      "Epoch 246 - Training Loss: 0.6622, Validation Loss: 0.7032\n",
      "Epoch 247 - Training Loss: 0.6883, Validation Loss: 0.6899\n",
      "Epoch 248 - Training Loss: 0.6713, Validation Loss: 0.6992\n",
      "Epoch 249 - Training Loss: 0.6896, Validation Loss: 0.7005\n",
      "Epoch 250 - Training Loss: 0.6896, Validation Loss: 0.6872\n",
      "Epoch 251 - Training Loss: 0.6791, Validation Loss: 0.6939\n",
      "Epoch 252 - Training Loss: 0.6974, Validation Loss: 0.6992\n",
      "Epoch 253 - Training Loss: 0.6628, Validation Loss: 0.6965\n",
      "Epoch 254 - Training Loss: 0.6804, Validation Loss: 0.7005\n",
      "Epoch 255 - Training Loss: 0.6883, Validation Loss: 0.6965\n",
      "Epoch 256 - Training Loss: 0.6804, Validation Loss: 0.6979\n",
      "Epoch 257 - Training Loss: 0.7065, Validation Loss: 0.6939\n",
      "Epoch 258 - Training Loss: 0.7078, Validation Loss: 0.6926\n",
      "Epoch 259 - Training Loss: 0.6890, Validation Loss: 0.7005\n",
      "Epoch 260 - Training Loss: 0.6883, Validation Loss: 0.6965\n",
      "Epoch 261 - Training Loss: 0.6973, Validation Loss: 0.6978\n",
      "Epoch 262 - Training Loss: 0.6894, Validation Loss: 0.6936\n",
      "Epoch 263 - Training Loss: 0.6791, Validation Loss: 0.6971\n",
      "Epoch 264 - Training Loss: 0.7065, Validation Loss: 0.6994\n",
      "Epoch 265 - Training Loss: 0.6883, Validation Loss: 0.6941\n",
      "Epoch 266 - Training Loss: 0.6700, Validation Loss: 0.6981\n",
      "Epoch 267 - Training Loss: 0.6802, Validation Loss: 0.6964\n",
      "Epoch 268 - Training Loss: 0.6974, Validation Loss: 0.6939\n",
      "Epoch 269 - Training Loss: 0.6883, Validation Loss: 0.6992\n",
      "Epoch 270 - Training Loss: 0.7065, Validation Loss: 0.6952\n",
      "Epoch 271 - Training Loss: 0.6883, Validation Loss: 0.7018\n",
      "Epoch 272 - Training Loss: 0.6883, Validation Loss: 0.7045\n",
      "Epoch 273 - Training Loss: 0.6791, Validation Loss: 0.7018\n",
      "Epoch 274 - Training Loss: 0.6883, Validation Loss: 0.6992\n",
      "Epoch 275 - Training Loss: 0.6791, Validation Loss: 0.6939\n",
      "Epoch 276 - Training Loss: 0.6883, Validation Loss: 0.7045\n",
      "Epoch 277 - Training Loss: 0.6883, Validation Loss: 0.6952\n",
      "Epoch 278 - Training Loss: 0.7052, Validation Loss: 0.6939\n",
      "Epoch 279 - Training Loss: 0.6974, Validation Loss: 0.6939\n",
      "Epoch 280 - Training Loss: 0.6974, Validation Loss: 0.7005\n",
      "Epoch 281 - Training Loss: 0.6791, Validation Loss: 0.7005\n",
      "Epoch 282 - Training Loss: 0.7156, Validation Loss: 0.7005\n",
      "Epoch 283 - Training Loss: 0.7157, Validation Loss: 0.7031\n",
      "Epoch 284 - Training Loss: 0.6883, Validation Loss: 0.7045\n",
      "Epoch 285 - Training Loss: 0.6609, Validation Loss: 0.7018\n",
      "Epoch 286 - Training Loss: 0.6700, Validation Loss: 0.6951\n",
      "Epoch 287 - Training Loss: 0.6776, Validation Loss: 0.6965\n",
      "Epoch 288 - Training Loss: 0.6791, Validation Loss: 0.6895\n",
      "Epoch 289 - Training Loss: 0.6792, Validation Loss: 0.6948\n",
      "Epoch 290 - Training Loss: 0.6690, Validation Loss: 0.6928\n",
      "Epoch 291 - Training Loss: 0.6791, Validation Loss: 0.6966\n",
      "Epoch 292 - Training Loss: 0.6963, Validation Loss: 0.6993\n",
      "Epoch 293 - Training Loss: 0.6872, Validation Loss: 0.6969\n",
      "Epoch 294 - Training Loss: 0.7052, Validation Loss: 0.6940\n",
      "Epoch 295 - Training Loss: 0.6870, Validation Loss: 0.6977\n",
      "Epoch 296 - Training Loss: 0.6961, Validation Loss: 0.7002\n",
      "Epoch 297 - Training Loss: 0.6779, Validation Loss: 0.6998\n",
      "Epoch 298 - Training Loss: 0.6883, Validation Loss: 0.6974\n",
      "Epoch 299 - Training Loss: 0.6596, Validation Loss: 0.6931\n",
      "Epoch 300 - Training Loss: 0.6873, Validation Loss: 0.6887\n",
      "Test predictions saved to C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\test_predictions_ELU_testing_almost_final.csv\n",
      "Test predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>Training Loss</td><td>▂▂▄▅▂▂▁▁▂▂▁▁▂▂▂▁▂▁▁▂▂▁▂▁▇███▂▁▁▂▁▂▁▂▂▁▁▁</td></tr><tr><td>Validation Loss</td><td>▂▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁█████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>300</td></tr><tr><td>Training Loss</td><td>0.68729</td></tr><tr><td>Validation Loss</td><td>0.68873</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-snowflake-37</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/qfvy5jgw' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/qfvy5jgw</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_165009-qfvy5jgw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Section 7: Argument Parsing and Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # argparse.Namespace로 인자를 직접 설정합니다.\n",
    "    args = argparse.Namespace(\n",
    "        train_csv_path=\"train.csv\",                # 실제 훈련 데이터 경로\n",
    "        test_csv_path=\"test.csv\",                  # 실제 테스트 데이터 경로\n",
    "        output_path=\"test_predictions_ELU_testing_almost_final.csv\",    # 테스트 예측 결과 파일 경로\n",
    "        wandb=True,                                # WandB 사용 여부 (True로 설정하면 WandB 사용)\n",
    "        batch_size=64,                             # 배치 사이즈\n",
    "        epochs=300,                                # 학습 에폭 수\n",
    "        learning_rate=0.001,                       # 학습률\n",
    "        hidden_units=[128, 64]                     # 은닉층 크기 리스트\n",
    "    )\n",
    "\n",
    "    # main 함수 호출\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81527b72-a58a-4f94-8cf7-82026fbe70cf",
   "metadata": {},
   "source": [
    "# Section 7: Argument Parsing and Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "746edbc4-599c-42b8-8535-01fcdcb4042a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\wandb\\run-20241025_165609-tids3uur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/tids3uur' target=\"_blank\">serene-waterfall-38</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/tids3uur' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/tids3uur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.7273, Validation Loss: 0.6761\n",
      "Epoch 2 - Training Loss: 0.7153, Validation Loss: 0.6725\n",
      "Epoch 3 - Training Loss: 0.6941, Validation Loss: 0.6906\n",
      "Epoch 4 - Training Loss: 0.7089, Validation Loss: 0.6802\n",
      "Epoch 5 - Training Loss: 0.7034, Validation Loss: 0.6774\n",
      "Epoch 6 - Training Loss: 0.7057, Validation Loss: 0.6814\n",
      "Epoch 7 - Training Loss: 0.6917, Validation Loss: 0.6690\n",
      "Epoch 8 - Training Loss: 0.6961, Validation Loss: 0.6610\n",
      "Epoch 9 - Training Loss: 0.6941, Validation Loss: 0.6803\n",
      "Epoch 10 - Training Loss: 0.6963, Validation Loss: 0.6726\n",
      "Epoch 11 - Training Loss: 0.7009, Validation Loss: 0.6564\n",
      "Epoch 12 - Training Loss: 0.6911, Validation Loss: 0.6676\n",
      "Epoch 13 - Training Loss: 0.6904, Validation Loss: 0.6686\n",
      "Epoch 14 - Training Loss: 0.6930, Validation Loss: 0.6695\n",
      "Epoch 15 - Training Loss: 0.6990, Validation Loss: 0.6764\n",
      "Epoch 16 - Training Loss: 0.7081, Validation Loss: 0.6731\n",
      "Epoch 17 - Training Loss: 0.6969, Validation Loss: 0.6705\n",
      "Epoch 18 - Training Loss: 0.6873, Validation Loss: 0.6806\n",
      "Epoch 19 - Training Loss: 0.6963, Validation Loss: 0.6670\n",
      "Epoch 20 - Training Loss: 0.7057, Validation Loss: 0.6747\n",
      "Epoch 21 - Training Loss: 0.6930, Validation Loss: 0.6750\n",
      "Epoch 22 - Training Loss: 0.6960, Validation Loss: 0.6832\n",
      "Epoch 23 - Training Loss: 0.6957, Validation Loss: 0.6758\n",
      "Epoch 24 - Training Loss: 0.6884, Validation Loss: 0.6639\n",
      "Epoch 25 - Training Loss: 0.6838, Validation Loss: 0.6645\n",
      "Epoch 26 - Training Loss: 0.6918, Validation Loss: 0.6762\n",
      "Epoch 27 - Training Loss: 0.6964, Validation Loss: 0.6684\n",
      "Epoch 28 - Training Loss: 0.6857, Validation Loss: 0.6808\n",
      "Epoch 29 - Training Loss: 0.7018, Validation Loss: 0.6808\n",
      "Epoch 30 - Training Loss: 0.6856, Validation Loss: 0.6671\n",
      "Epoch 31 - Training Loss: 0.6884, Validation Loss: 0.6648\n",
      "Epoch 32 - Training Loss: 0.6912, Validation Loss: 0.6765\n",
      "Epoch 33 - Training Loss: 0.6821, Validation Loss: 0.6779\n",
      "Epoch 34 - Training Loss: 0.6943, Validation Loss: 0.6769\n",
      "Epoch 35 - Training Loss: 0.6963, Validation Loss: 0.6676\n",
      "Epoch 36 - Training Loss: 0.6928, Validation Loss: 0.6678\n",
      "Epoch 37 - Training Loss: 0.6894, Validation Loss: 0.6693\n",
      "Epoch 38 - Training Loss: 0.6894, Validation Loss: 0.6776\n",
      "Epoch 39 - Training Loss: 0.6972, Validation Loss: 0.6829\n",
      "Epoch 40 - Training Loss: 0.6875, Validation Loss: 0.6749\n",
      "Epoch 41 - Training Loss: 0.6918, Validation Loss: 0.6828\n",
      "Epoch 42 - Training Loss: 0.7008, Validation Loss: 0.6825\n",
      "Epoch 43 - Training Loss: 0.6851, Validation Loss: 0.6866\n",
      "Epoch 44 - Training Loss: 0.7116, Validation Loss: 0.6597\n",
      "Epoch 45 - Training Loss: 0.6948, Validation Loss: 0.6774\n",
      "Epoch 46 - Training Loss: 0.6874, Validation Loss: 0.6721\n",
      "Epoch 47 - Training Loss: 0.6791, Validation Loss: 0.6757\n",
      "Epoch 48 - Training Loss: 0.6894, Validation Loss: 0.6863\n",
      "Epoch 49 - Training Loss: 0.6940, Validation Loss: 0.6809\n",
      "Epoch 50 - Training Loss: 0.6882, Validation Loss: 0.6653\n",
      "Epoch 51 - Training Loss: 0.6931, Validation Loss: 0.6761\n",
      "Epoch 52 - Training Loss: 0.6975, Validation Loss: 0.6661\n",
      "Epoch 53 - Training Loss: 0.6945, Validation Loss: 0.6790\n",
      "Epoch 54 - Training Loss: 0.6828, Validation Loss: 0.6800\n",
      "Epoch 55 - Training Loss: 0.7076, Validation Loss: 0.6715\n",
      "Epoch 56 - Training Loss: 0.6767, Validation Loss: 0.6615\n",
      "Epoch 57 - Training Loss: 0.6870, Validation Loss: 0.6796\n",
      "Epoch 58 - Training Loss: 0.7089, Validation Loss: 0.6636\n",
      "Epoch 59 - Training Loss: 0.6819, Validation Loss: 0.6781\n",
      "Epoch 60 - Training Loss: 0.7023, Validation Loss: 0.6713\n",
      "Epoch 61 - Training Loss: 0.6905, Validation Loss: 0.6822\n",
      "Epoch 62 - Training Loss: 0.6853, Validation Loss: 0.6679\n",
      "Epoch 63 - Training Loss: 0.6937, Validation Loss: 0.6788\n",
      "Epoch 64 - Training Loss: 0.6815, Validation Loss: 0.6811\n",
      "Epoch 65 - Training Loss: 0.7043, Validation Loss: 0.6753\n",
      "Epoch 66 - Training Loss: 0.6887, Validation Loss: 0.6779\n",
      "Epoch 67 - Training Loss: 0.6902, Validation Loss: 0.6794\n",
      "Epoch 68 - Training Loss: 0.6845, Validation Loss: 0.6679\n",
      "Epoch 69 - Training Loss: 0.6992, Validation Loss: 0.6908\n",
      "Epoch 70 - Training Loss: 0.6842, Validation Loss: 0.6791\n",
      "Epoch 71 - Training Loss: 0.6893, Validation Loss: 0.6735\n",
      "Epoch 72 - Training Loss: 0.6766, Validation Loss: 0.6786\n",
      "Epoch 73 - Training Loss: 0.6838, Validation Loss: 0.6823\n",
      "Epoch 74 - Training Loss: 0.6879, Validation Loss: 0.6680\n",
      "Epoch 75 - Training Loss: 0.6892, Validation Loss: 0.6715\n",
      "Epoch 76 - Training Loss: 0.6825, Validation Loss: 0.6715\n",
      "Epoch 77 - Training Loss: 0.6918, Validation Loss: 0.6750\n",
      "Epoch 78 - Training Loss: 0.6800, Validation Loss: 0.6718\n",
      "Epoch 79 - Training Loss: 0.6924, Validation Loss: 0.6724\n",
      "Epoch 80 - Training Loss: 0.6895, Validation Loss: 0.6610\n",
      "Epoch 81 - Training Loss: 0.6992, Validation Loss: 0.6715\n",
      "Epoch 82 - Training Loss: 0.6814, Validation Loss: 0.6715\n",
      "Epoch 83 - Training Loss: 0.6929, Validation Loss: 0.6574\n",
      "Epoch 84 - Training Loss: 0.6881, Validation Loss: 0.6752\n",
      "Epoch 85 - Training Loss: 0.6828, Validation Loss: 0.6764\n",
      "Epoch 86 - Training Loss: 0.6852, Validation Loss: 0.6659\n",
      "Epoch 87 - Training Loss: 0.6888, Validation Loss: 0.6832\n",
      "Epoch 88 - Training Loss: 0.6924, Validation Loss: 0.6699\n",
      "Epoch 89 - Training Loss: 0.6891, Validation Loss: 0.6694\n",
      "Epoch 90 - Training Loss: 0.6738, Validation Loss: 0.6722\n",
      "Epoch 91 - Training Loss: 0.6910, Validation Loss: 0.6730\n",
      "Epoch 92 - Training Loss: 0.6941, Validation Loss: 0.6898\n",
      "Epoch 93 - Training Loss: 0.6904, Validation Loss: 0.6672\n",
      "Epoch 94 - Training Loss: 0.6936, Validation Loss: 0.6729\n",
      "Epoch 95 - Training Loss: 0.6913, Validation Loss: 0.6768\n",
      "Epoch 96 - Training Loss: 0.6816, Validation Loss: 0.6862\n",
      "Epoch 97 - Training Loss: 0.6915, Validation Loss: 0.6856\n",
      "Epoch 98 - Training Loss: 0.6781, Validation Loss: 0.6827\n",
      "Epoch 99 - Training Loss: 0.6865, Validation Loss: 0.6827\n",
      "Epoch 100 - Training Loss: 0.6898, Validation Loss: 0.6770\n",
      "Epoch 101 - Training Loss: 0.6810, Validation Loss: 0.6849\n",
      "Epoch 102 - Training Loss: 0.7571, Validation Loss: 0.6918\n",
      "Epoch 103 - Training Loss: 0.6997, Validation Loss: 0.6752\n",
      "Epoch 104 - Training Loss: 0.6824, Validation Loss: 0.6751\n",
      "Epoch 105 - Training Loss: 0.6901, Validation Loss: 0.6644\n",
      "Epoch 106 - Training Loss: 0.6868, Validation Loss: 0.6609\n",
      "Epoch 107 - Training Loss: 0.6914, Validation Loss: 0.6680\n",
      "Epoch 108 - Training Loss: 0.6882, Validation Loss: 0.6716\n",
      "Epoch 109 - Training Loss: 0.6923, Validation Loss: 0.6646\n",
      "Epoch 110 - Training Loss: 0.6899, Validation Loss: 0.6681\n",
      "Epoch 111 - Training Loss: 0.6892, Validation Loss: 0.6655\n",
      "Epoch 112 - Training Loss: 0.6854, Validation Loss: 0.6747\n",
      "Epoch 113 - Training Loss: 0.6740, Validation Loss: 0.6723\n",
      "Epoch 114 - Training Loss: 0.6846, Validation Loss: 0.6718\n",
      "Epoch 115 - Training Loss: 0.6832, Validation Loss: 0.6680\n",
      "Epoch 116 - Training Loss: 0.7002, Validation Loss: 0.6752\n",
      "Epoch 117 - Training Loss: 0.6805, Validation Loss: 0.6684\n",
      "Epoch 118 - Training Loss: 0.6898, Validation Loss: 0.6580\n",
      "Epoch 119 - Training Loss: 0.6905, Validation Loss: 0.6792\n",
      "Epoch 120 - Training Loss: 0.6916, Validation Loss: 0.6647\n",
      "Epoch 121 - Training Loss: 0.6857, Validation Loss: 0.6794\n",
      "Epoch 122 - Training Loss: 0.6909, Validation Loss: 0.6554\n",
      "Epoch 123 - Training Loss: 0.6864, Validation Loss: 0.6804\n",
      "Epoch 124 - Training Loss: 0.6836, Validation Loss: 0.6684\n",
      "Epoch 125 - Training Loss: 0.6863, Validation Loss: 0.6645\n",
      "Epoch 126 - Training Loss: 0.6857, Validation Loss: 0.6716\n",
      "Epoch 127 - Training Loss: 0.6792, Validation Loss: 0.6680\n",
      "Epoch 128 - Training Loss: 0.6930, Validation Loss: 0.6752\n",
      "Epoch 129 - Training Loss: 0.6921, Validation Loss: 0.6766\n",
      "Epoch 130 - Training Loss: 0.6840, Validation Loss: 0.6822\n",
      "Epoch 131 - Training Loss: 0.6838, Validation Loss: 0.6664\n",
      "Epoch 132 - Training Loss: 0.6932, Validation Loss: 0.6748\n",
      "Epoch 133 - Training Loss: 0.6750, Validation Loss: 0.6878\n",
      "Epoch 134 - Training Loss: 0.6937, Validation Loss: 0.6829\n",
      "Epoch 135 - Training Loss: 0.6871, Validation Loss: 0.6695\n",
      "Epoch 136 - Training Loss: 0.6794, Validation Loss: 0.6729\n",
      "Epoch 137 - Training Loss: 0.6709, Validation Loss: 0.6681\n",
      "Epoch 138 - Training Loss: 0.6811, Validation Loss: 0.6705\n",
      "Epoch 139 - Training Loss: 0.6837, Validation Loss: 0.6816\n",
      "Epoch 140 - Training Loss: 0.6756, Validation Loss: 0.6679\n",
      "Epoch 141 - Training Loss: 0.6742, Validation Loss: 0.6791\n",
      "Epoch 142 - Training Loss: 0.6791, Validation Loss: 0.6642\n",
      "Epoch 143 - Training Loss: 0.6665, Validation Loss: 0.6645\n",
      "Epoch 144 - Training Loss: 0.6903, Validation Loss: 0.6851\n",
      "Epoch 145 - Training Loss: 0.6801, Validation Loss: 0.6805\n",
      "Epoch 146 - Training Loss: 0.6812, Validation Loss: 0.6765\n",
      "Epoch 147 - Training Loss: 0.6781, Validation Loss: 0.6720\n",
      "Epoch 148 - Training Loss: 0.6886, Validation Loss: 0.6693\n",
      "Epoch 149 - Training Loss: 0.6731, Validation Loss: 0.6729\n",
      "Epoch 150 - Training Loss: 0.6800, Validation Loss: 0.6790\n",
      "Epoch 151 - Training Loss: 0.6872, Validation Loss: 0.6666\n",
      "Epoch 152 - Training Loss: 0.6803, Validation Loss: 0.6451\n",
      "Epoch 153 - Training Loss: 0.7117, Validation Loss: 0.6675\n",
      "Epoch 154 - Training Loss: 0.6739, Validation Loss: 0.6823\n",
      "Epoch 155 - Training Loss: 0.6868, Validation Loss: 0.6781\n",
      "Epoch 156 - Training Loss: 0.6655, Validation Loss: 0.6593\n",
      "Epoch 157 - Training Loss: 0.6739, Validation Loss: 0.6626\n",
      "Epoch 158 - Training Loss: 0.6750, Validation Loss: 0.6738\n",
      "Epoch 159 - Training Loss: 0.6869, Validation Loss: 0.6709\n",
      "Epoch 160 - Training Loss: 0.6724, Validation Loss: 0.6650\n",
      "Epoch 161 - Training Loss: 0.6751, Validation Loss: 0.6811\n",
      "Epoch 162 - Training Loss: 0.6727, Validation Loss: 0.6638\n",
      "Epoch 163 - Training Loss: 0.6793, Validation Loss: 0.6775\n",
      "Epoch 164 - Training Loss: 0.6733, Validation Loss: 0.6787\n",
      "Epoch 165 - Training Loss: 0.6810, Validation Loss: 0.6565\n",
      "Epoch 166 - Training Loss: 0.6776, Validation Loss: 0.6762\n",
      "Epoch 167 - Training Loss: 0.6680, Validation Loss: 0.6566\n",
      "Epoch 168 - Training Loss: 0.6842, Validation Loss: 0.6680\n",
      "Epoch 169 - Training Loss: 0.6883, Validation Loss: 0.6688\n",
      "Epoch 170 - Training Loss: 0.6679, Validation Loss: 0.6684\n",
      "Epoch 171 - Training Loss: 0.6708, Validation Loss: 0.6918\n",
      "Epoch 172 - Training Loss: 0.6777, Validation Loss: 0.6572\n",
      "Epoch 173 - Training Loss: 0.6782, Validation Loss: 0.6642\n",
      "Epoch 174 - Training Loss: 0.7082, Validation Loss: 0.9137\n",
      "Epoch 175 - Training Loss: 0.6799, Validation Loss: 0.6680\n",
      "Epoch 176 - Training Loss: 0.6891, Validation Loss: 0.6715\n",
      "Epoch 177 - Training Loss: 0.6838, Validation Loss: 0.6680\n",
      "Epoch 178 - Training Loss: 0.6751, Validation Loss: 0.6573\n",
      "Epoch 179 - Training Loss: 0.6833, Validation Loss: 0.6644\n",
      "Epoch 180 - Training Loss: 0.6909, Validation Loss: 0.6702\n",
      "Epoch 181 - Training Loss: 0.6894, Validation Loss: 0.6669\n",
      "Epoch 182 - Training Loss: 0.6810, Validation Loss: 0.6536\n",
      "Epoch 183 - Training Loss: 0.6819, Validation Loss: 0.6654\n",
      "Epoch 184 - Training Loss: 0.6812, Validation Loss: 0.6579\n",
      "Epoch 185 - Training Loss: 0.6862, Validation Loss: 0.6680\n",
      "Epoch 186 - Training Loss: 0.6846, Validation Loss: 0.6644\n",
      "Epoch 187 - Training Loss: 0.6846, Validation Loss: 0.6822\n",
      "Epoch 188 - Training Loss: 0.6880, Validation Loss: 0.6608\n",
      "Epoch 189 - Training Loss: 0.7009, Validation Loss: 0.6715\n",
      "Epoch 190 - Training Loss: 0.6909, Validation Loss: 0.6680\n",
      "Epoch 191 - Training Loss: 0.6802, Validation Loss: 0.6680\n",
      "Epoch 192 - Training Loss: 0.6915, Validation Loss: 0.6641\n",
      "Epoch 193 - Training Loss: 0.6733, Validation Loss: 0.6858\n",
      "Epoch 194 - Training Loss: 0.6699, Validation Loss: 0.6787\n",
      "Epoch 195 - Training Loss: 0.6683, Validation Loss: 0.6633\n",
      "Epoch 196 - Training Loss: 0.6760, Validation Loss: 0.6805\n",
      "Epoch 197 - Training Loss: 0.6820, Validation Loss: 0.6735\n",
      "Epoch 198 - Training Loss: 0.6747, Validation Loss: 0.6676\n",
      "Epoch 199 - Training Loss: 0.6770, Validation Loss: 0.6768\n",
      "Epoch 200 - Training Loss: 0.6658, Validation Loss: 0.6853\n",
      "Epoch 201 - Training Loss: 0.7453, Validation Loss: 0.9156\n",
      "Epoch 202 - Training Loss: 0.6888, Validation Loss: 0.6713\n",
      "Epoch 203 - Training Loss: 0.6904, Validation Loss: 0.6654\n",
      "Epoch 204 - Training Loss: 0.6826, Validation Loss: 0.6786\n",
      "Epoch 205 - Training Loss: 0.6800, Validation Loss: 0.6673\n",
      "Epoch 206 - Training Loss: 0.6800, Validation Loss: 0.6598\n",
      "Epoch 207 - Training Loss: 0.6749, Validation Loss: 0.6628\n",
      "Epoch 208 - Training Loss: 0.6772, Validation Loss: 0.6699\n",
      "Epoch 209 - Training Loss: 0.6885, Validation Loss: 0.6664\n",
      "Epoch 210 - Training Loss: 0.6849, Validation Loss: 0.6629\n",
      "Epoch 211 - Training Loss: 0.6653, Validation Loss: 0.6521\n",
      "Epoch 212 - Training Loss: 0.6903, Validation Loss: 0.6664\n",
      "Epoch 213 - Training Loss: 0.6702, Validation Loss: 0.6664\n",
      "Epoch 214 - Training Loss: 0.6832, Validation Loss: 0.6771\n",
      "Epoch 215 - Training Loss: 0.6793, Validation Loss: 0.6539\n",
      "Epoch 216 - Training Loss: 0.6746, Validation Loss: 0.6628\n",
      "Epoch 217 - Training Loss: 0.6912, Validation Loss: 0.6607\n",
      "Epoch 218 - Training Loss: 0.6692, Validation Loss: 0.6684\n",
      "Epoch 219 - Training Loss: 0.6783, Validation Loss: 0.6720\n",
      "Epoch 220 - Training Loss: 0.6876, Validation Loss: 0.6907\n",
      "Epoch 221 - Training Loss: 0.6861, Validation Loss: 0.6596\n",
      "Epoch 222 - Training Loss: 0.6867, Validation Loss: 0.6667\n",
      "Epoch 223 - Training Loss: 0.6855, Validation Loss: 0.6788\n",
      "Epoch 224 - Training Loss: 0.6764, Validation Loss: 0.6740\n",
      "Epoch 225 - Training Loss: 0.6818, Validation Loss: 0.6680\n",
      "Epoch 226 - Training Loss: 0.6769, Validation Loss: 0.6679\n",
      "Epoch 227 - Training Loss: 0.6867, Validation Loss: 0.6640\n",
      "Epoch 228 - Training Loss: 0.6812, Validation Loss: 0.6735\n",
      "Epoch 229 - Training Loss: 0.6749, Validation Loss: 0.6596\n",
      "Epoch 230 - Training Loss: 0.6766, Validation Loss: 0.6592\n",
      "Epoch 231 - Training Loss: 0.6722, Validation Loss: 0.6782\n",
      "Epoch 232 - Training Loss: 0.6792, Validation Loss: 0.6705\n",
      "Epoch 233 - Training Loss: 0.6860, Validation Loss: 0.6640\n",
      "Epoch 234 - Training Loss: 0.6700, Validation Loss: 0.6715\n",
      "Epoch 235 - Training Loss: 0.6771, Validation Loss: 0.6770\n",
      "Epoch 236 - Training Loss: 0.6758, Validation Loss: 0.6702\n",
      "Epoch 237 - Training Loss: 0.6768, Validation Loss: 0.6718\n",
      "Epoch 238 - Training Loss: 0.6859, Validation Loss: 0.6617\n",
      "Epoch 239 - Training Loss: 0.6713, Validation Loss: 0.6629\n",
      "Epoch 240 - Training Loss: 0.6725, Validation Loss: 0.6687\n",
      "Epoch 241 - Training Loss: 0.6750, Validation Loss: 0.6594\n",
      "Epoch 242 - Training Loss: 0.6583, Validation Loss: 0.6687\n",
      "Epoch 243 - Training Loss: 0.6807, Validation Loss: 0.6588\n",
      "Epoch 244 - Training Loss: 0.6769, Validation Loss: 0.6838\n",
      "Epoch 245 - Training Loss: 0.6893, Validation Loss: 0.6678\n",
      "Epoch 246 - Training Loss: 0.6749, Validation Loss: 0.6670\n",
      "Epoch 247 - Training Loss: 0.6719, Validation Loss: 0.6663\n",
      "Epoch 248 - Training Loss: 0.6815, Validation Loss: 0.6751\n",
      "Epoch 249 - Training Loss: 0.6787, Validation Loss: 0.6715\n",
      "Epoch 250 - Training Loss: 0.6792, Validation Loss: 0.6712\n",
      "Test predictions saved to C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\test_predictions_ELU_testing_almost_final.csv\n",
      "Test predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇██████</td></tr><tr><td>Training Loss</td><td>█▅▅▄▄▃▄▄▅▃▃▅▄▄▄▃▄▃▃▄▃▃▂▂▃▆▃▃▃▄▂▁▃▃▂▃▃▃▂▃</td></tr><tr><td>Validation Loss</td><td>█▄▁▆▅▆▅▄▂▆▃▄▅▅▆▅▄▅▅▃▃▃▃▄▃▁▃▃▂▃▃▂▂▃▄▃▄▂▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>250</td></tr><tr><td>Training Loss</td><td>0.67918</td></tr><tr><td>Validation Loss</td><td>0.67115</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-waterfall-38</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/tids3uur' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/tids3uur</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_165609-tids3uur\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Section 7: Argument Parsing and Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # argparse.Namespace로 인자를 직접 설정합니다.\n",
    "    args = argparse.Namespace(\n",
    "        train_csv_path=\"train.csv\",                # 실제 훈련 데이터 경로\n",
    "        test_csv_path=\"test.csv\",                  # 실제 테스트 데이터 경로\n",
    "        output_path=\"test_predictions_ELU_testing_almost_final.csv\",    # 테스트 예측 결과 파일 경로\n",
    "        wandb=True,                                # WandB 사용 여부 (True로 설정하면 WandB 사용)\n",
    "        batch_size=32,                             # 배치 사이즈\n",
    "        epochs=250,                                # 학습 에폭 수\n",
    "        learning_rate=0.0005,                       # 학습률\n",
    "        hidden_units=[128, 64]                     # 은닉층 크기 리스트\n",
    "    )\n",
    "\n",
    "    # main 함수 호출\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea59f0a-218e-4142-a581-a82a7244504f",
   "metadata": {},
   "source": [
    "![competition78](./78Scores_leaderboard2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1a4b8-422b-4e28-af0e-6e8cb0166b3f",
   "metadata": {},
   "source": [
    "# 새로 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a3559-efb2-442f-b463-47eece214eb7",
   "metadata": {},
   "source": [
    "# Section 1: Imports and Dataset Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7677640a-9686-4957-b99e-06fc71035c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and Dataset Class Definitions\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf91bd-4610-4d78-9c1b-725f0cb67e99",
   "metadata": {},
   "source": [
    "# Section 2: # TitanicDataset Class for Train and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "94716d6f-2894-4581-8549-444a0aceed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TitanicDataset Class for Train and Validation Data\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1942ad-b7d7-47b1-a641-dc97209d6000",
   "metadata": {},
   "source": [
    "# Section 3: TitanicTestDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8c537923-2854-4e34-bf4c-e7d10f1920ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X, passenger_ids):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.passenger_ids = passenger_ids  # PassengerId 추가\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        passenger_id = self.passenger_ids[idx]  # PassengerId 반환\n",
    "        return {'input': feature, 'PassengerId': passenger_id}  # PassengerId와 입력 특징 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0e341-a7d3-42d9-8151-43d71190bd9b",
   "metadata": {},
   "source": [
    "# Section 4: get_preprocessed_dataset_1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "33191553-c6b6-4793-a871-4ee474db2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(df):\n",
    "    # 성별 인코딩 (male=0, female=1)\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_2(df):\n",
    "    # 결측값 처리 (Age의 결측값을 평균으로 채움)\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_3(df):\n",
    "    # 결측값 처리 (Fare의 결측값을 평균으로 채움)\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_4(df):\n",
    "    # 불필요한 열 제거, 단 PassengerId는 제거하지 않음\n",
    "    columns_to_drop = ['Name', 'Ticket', 'Cabin', 'Embarked']  # 'PassengerId' 제외\n",
    "    df = df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_5(df):\n",
    "    # Pclass를 범주형에서 연속형으로 처리\n",
    "    df['Pclass'] = df['Pclass'].astype(float)\n",
    "    return df\n",
    "\n",
    "def get_preprocessed_dataset_6(df):\n",
    "    # 정규화 (필요한 경우)\n",
    "    scaler = StandardScaler()\n",
    "    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6353a-3d7e-46ac-9a50-9923a3dd3ae3",
   "metadata": {},
   "source": [
    "# Section 5 : get_preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a328c0a5-c8e5-4284-81e9-76773735ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Preprocessing\n",
    "def get_preprocessed_dataset():\n",
    "    CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    \n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    \n",
    "    # Call pre-processing functions (define as needed)\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45ddd0-5acf-45d2-aca4-b8f705e0dd97",
   "metadata": {},
   "source": [
    "# Section 6 : Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b00a749a-4223-4c23-986d-4fcf66141d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Model Definition version 2\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),  # Dropout 추가\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),  # Dropout 추가\n",
    "            nn.Linear(64, n_output),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1fa50e25-3037-471b-8ad3-4f1a13995e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Training and Evaluation Loops_v2_wandB에 추가하기\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_data_loader:\n",
    "            inputs, targets = batch['input'], batch['target']\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation Loss 계산\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader:\n",
    "                inputs, targets = batch['input'], batch['target']\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                validation_loss += loss.item()\n",
    "        \n",
    "        validation_loss /= len(validation_data_loader)\n",
    "\n",
    "        # WandB에 기록\n",
    "        wandb.log({\"Training Loss\": train_loss, \"Validation Loss\": validation_loss, \"Epoch\": epoch})\n",
    "\n",
    "        print(f\"Epoch {epoch} - Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afc5ae-ec8d-4fc9-b50e-52f84b29563c",
   "metadata": {},
   "source": [
    "# Section 7 : predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "95ffb94e-4160-4ec7-af6e-8285e3aa26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_test version 5\n",
    "# Section 5: Model Testing and Predictio\n",
    "import pandas as pd\n",
    "\n",
    "# Model Testing and Prediction\n",
    "def predict_test(model, test_data_loader, output_path=\"test_predictions_ELU_testing_almost_final.csv\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    passenger_ids = []  # PassengerId 저장을 위한 리스트\n",
    "\n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            inputs = batch['input']\n",
    "            passenger_ids_batch = batch['PassengerId']  # PassengerId 가져오기\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # 예측 값과 PassengerId 저장\n",
    "            predictions.extend(predicted_labels.cpu().numpy())  # Tensor에서 numpy로 변환\n",
    "            passenger_ids.extend(passenger_ids_batch.cpu().numpy())  # Tensor에서 numpy로 변환\n",
    "\n",
    "    # 예측 결과를 DataFrame으로 변환\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"PassengerId\": passenger_ids,\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "    # DataFrame을 CSV 파일로 저장\n",
    "    preds_df.to_csv(output_path, index=False)\n",
    "    print(f\"Test predictions saved to {os.path.abspath(output_path)}\")  # 파일의 절대 경로 출력\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64aa37b-1479-40e4-b2b3-1195882324e9",
   "metadata": {},
   "source": [
    "# Section 8 : main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9751aef4-6d46-4d32-b235-236e58270c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3\n",
    "def main(args):\n",
    "    wandb.init(project=\"titanic_model_training\", config={\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"learning_rate\": args.learning_rate\n",
    "    })\n",
    "\n",
    "    # 전처리된 데이터셋 로드\n",
    "    all_df = get_preprocessed_dataset()\n",
    "    \n",
    "    # 훈련, 검증, 테스트 데이터셋 분리\n",
    "    train_df = all_df[all_df['Survived'].notna()]  # Survived 값이 있는 행을 훈련용 데이터로 사용\n",
    "    test_df = all_df[all_df['Survived'].isna()]    # Survived 값이 없는 행을 테스트 데이터로 사용\n",
    "    \n",
    "    # 특징(X)와 타겟(y) 분리\n",
    "    X_train = train_df.drop(columns=['Survived']).values\n",
    "    y_train = train_df['Survived'].values\n",
    "\n",
    "    X_test = test_df.drop(columns=['Survived']).values\n",
    "    passenger_ids_test = test_df['PassengerId'].values  # 테스트 데이터에서 PassengerId 가져오기\n",
    "\n",
    "    # Dataset 생성\n",
    "    full_train_dataset = TitanicDataset(X_train, y_train)\n",
    "    \n",
    "    # 훈련/검증 데이터 분리 (예: 80% 훈련, 20% 검증)\n",
    "    train_size = int(0.8 * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # 테스트 데이터셋에 PassengerId 포함\n",
    "    test_dataset = TitanicTestDataset(X_test, passenger_ids_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(validation_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model = MyModel(n_input=X_train.shape[1], n_output=2)  # 출력 크기를 2로 설정해 이진 분류 수행\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 훈련 루프 실행\n",
    "    training_loop(model, optimizer, train_data_loader, validation_data_loader)\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 수행 및 결과 출력\n",
    "    test_predictions = predict_test(model, test_data_loader)\n",
    "    print(f\"Test predictions: {test_predictions}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50817c4-4c3e-45cc-ac8d-4338acb8bf76",
   "metadata": {},
   "source": [
    "# Section 9: Argument Parsing and Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2becb91-9703-423d-8f86-55fdc2c480f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # argparse.Namespace로 인자를 직접 설정합니다.\n",
    "    args = argparse.Namespace(\n",
    "        train_csv_path=\"train.csv\",                # 실제 훈련 데이터 경로\n",
    "        test_csv_path=\"test.csv\",                  # 실제 테스트 데이터 경로\n",
    "        output_path=\"test_predictions_ELU_testing_almost_final.csv\",    # 테스트 예측 결과 파일 경로\n",
    "        wandb=True,                                # WandB 사용 여부 (True로 설정하면 WandB 사용)\n",
    "        batch_size=64,                             # 배치 사이즈\n",
    "        epochs=300,                                # 학습 에폭 수\n",
    "        learning_rate=0.001,                       # 학습률\n",
    "        hidden_units=[128, 64]                     # 은닉층 크기 리스트\n",
    "    )\n",
    "\n",
    "    # main 함수 호출\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1fedcbc1-08ed-4df9-8a03-0e264e93f407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\wandb\\run-20241025_173251-ynnkjb14</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/ynnkjb14' target=\"_blank\">dandy-armadillo-39</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/ynnkjb14' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/ynnkjb14</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.6978, Validation Loss: 0.6836\n",
      "Epoch 2 - Training Loss: 0.6960, Validation Loss: 0.7085\n",
      "Epoch 3 - Training Loss: 0.7061, Validation Loss: 0.6979\n",
      "Epoch 4 - Training Loss: 0.6939, Validation Loss: 0.6992\n",
      "Epoch 5 - Training Loss: 0.6979, Validation Loss: 0.6882\n",
      "Epoch 6 - Training Loss: 0.6931, Validation Loss: 0.6866\n",
      "Epoch 7 - Training Loss: 0.6958, Validation Loss: 0.6913\n",
      "Epoch 8 - Training Loss: 0.6967, Validation Loss: 0.6953\n",
      "Epoch 9 - Training Loss: 0.6991, Validation Loss: 0.6897\n",
      "Epoch 10 - Training Loss: 0.6909, Validation Loss: 0.6949\n",
      "Epoch 11 - Training Loss: 0.6798, Validation Loss: 0.6794\n",
      "Epoch 12 - Training Loss: 0.6916, Validation Loss: 0.6879\n",
      "Epoch 13 - Training Loss: 0.6968, Validation Loss: 0.6849\n",
      "Epoch 14 - Training Loss: 0.6976, Validation Loss: 0.6862\n",
      "Epoch 15 - Training Loss: 0.7814, Validation Loss: 0.6785\n",
      "Epoch 16 - Training Loss: 0.7337, Validation Loss: 0.6963\n",
      "Epoch 17 - Training Loss: 0.6850, Validation Loss: 0.6927\n",
      "Epoch 18 - Training Loss: 0.6964, Validation Loss: 0.6890\n",
      "Epoch 19 - Training Loss: 0.6961, Validation Loss: 0.6961\n",
      "Epoch 20 - Training Loss: 0.6926, Validation Loss: 0.6855\n",
      "Epoch 21 - Training Loss: 0.6944, Validation Loss: 0.6999\n",
      "Epoch 22 - Training Loss: 0.6912, Validation Loss: 0.7004\n",
      "Epoch 23 - Training Loss: 0.6893, Validation Loss: 0.7078\n",
      "Epoch 24 - Training Loss: 0.6866, Validation Loss: 0.6902\n",
      "Epoch 25 - Training Loss: 0.6910, Validation Loss: 0.6903\n",
      "Epoch 26 - Training Loss: 0.6991, Validation Loss: 0.6899\n",
      "Epoch 27 - Training Loss: 0.6870, Validation Loss: 0.6931\n",
      "Epoch 28 - Training Loss: 0.6946, Validation Loss: 0.6888\n",
      "Epoch 29 - Training Loss: 0.6855, Validation Loss: 0.6961\n",
      "Epoch 30 - Training Loss: 0.6802, Validation Loss: 0.7088\n",
      "Epoch 31 - Training Loss: 0.6979, Validation Loss: 0.6787\n",
      "Epoch 32 - Training Loss: 0.6890, Validation Loss: 0.7004\n",
      "Epoch 33 - Training Loss: 0.6854, Validation Loss: 0.6953\n",
      "Epoch 34 - Training Loss: 0.6937, Validation Loss: 0.6926\n",
      "Epoch 35 - Training Loss: 0.6829, Validation Loss: 0.6956\n",
      "Epoch 36 - Training Loss: 0.6897, Validation Loss: 0.6940\n",
      "Epoch 37 - Training Loss: 0.6838, Validation Loss: 0.6906\n",
      "Epoch 38 - Training Loss: 0.6803, Validation Loss: 0.6963\n",
      "Epoch 39 - Training Loss: 0.6961, Validation Loss: 0.6800\n",
      "Epoch 40 - Training Loss: 0.6923, Validation Loss: 0.6908\n",
      "Epoch 41 - Training Loss: 0.6861, Validation Loss: 0.7014\n",
      "Epoch 42 - Training Loss: 0.6871, Validation Loss: 0.7017\n",
      "Epoch 43 - Training Loss: 0.6849, Validation Loss: 0.6998\n",
      "Epoch 44 - Training Loss: 0.6910, Validation Loss: 0.6887\n",
      "Epoch 45 - Training Loss: 0.6849, Validation Loss: 0.6741\n",
      "Epoch 46 - Training Loss: 0.6945, Validation Loss: 0.6895\n",
      "Epoch 47 - Training Loss: 0.6766, Validation Loss: 0.6941\n",
      "Epoch 48 - Training Loss: 0.6861, Validation Loss: 0.6838\n",
      "Epoch 49 - Training Loss: 0.6899, Validation Loss: 0.6896\n",
      "Epoch 50 - Training Loss: 0.6838, Validation Loss: 0.6751\n",
      "Epoch 51 - Training Loss: 0.6800, Validation Loss: 0.7017\n",
      "Epoch 52 - Training Loss: 0.6963, Validation Loss: 0.6921\n",
      "Epoch 53 - Training Loss: 0.6875, Validation Loss: 0.6922\n",
      "Epoch 54 - Training Loss: 0.6811, Validation Loss: 0.6921\n",
      "Epoch 55 - Training Loss: 0.6858, Validation Loss: 0.6849\n",
      "Epoch 56 - Training Loss: 0.6939, Validation Loss: 0.6871\n",
      "Epoch 57 - Training Loss: 0.6836, Validation Loss: 0.6933\n",
      "Epoch 58 - Training Loss: 0.6800, Validation Loss: 0.6924\n",
      "Epoch 59 - Training Loss: 0.6886, Validation Loss: 0.6943\n",
      "Epoch 60 - Training Loss: 0.6768, Validation Loss: 0.6814\n",
      "Epoch 61 - Training Loss: 0.6780, Validation Loss: 0.6769\n",
      "Epoch 62 - Training Loss: 0.6755, Validation Loss: 0.6956\n",
      "Epoch 63 - Training Loss: 0.6838, Validation Loss: 0.6647\n",
      "Epoch 64 - Training Loss: 0.6790, Validation Loss: 0.6813\n",
      "Epoch 65 - Training Loss: 0.6896, Validation Loss: 0.6800\n",
      "Epoch 66 - Training Loss: 0.6792, Validation Loss: 0.6819\n",
      "Epoch 67 - Training Loss: 0.6884, Validation Loss: 0.6911\n",
      "Epoch 68 - Training Loss: 0.6878, Validation Loss: 0.6885\n",
      "Epoch 69 - Training Loss: 0.6767, Validation Loss: 0.6880\n",
      "Epoch 70 - Training Loss: 0.6876, Validation Loss: 0.6815\n",
      "Epoch 71 - Training Loss: 0.6795, Validation Loss: 0.6810\n",
      "Epoch 72 - Training Loss: 0.6899, Validation Loss: 0.6850\n",
      "Epoch 73 - Training Loss: 0.6864, Validation Loss: 0.6893\n",
      "Epoch 74 - Training Loss: 0.6918, Validation Loss: 0.6919\n",
      "Epoch 75 - Training Loss: 0.6874, Validation Loss: 0.6863\n",
      "Epoch 76 - Training Loss: 0.6900, Validation Loss: 0.6933\n",
      "Epoch 77 - Training Loss: 0.6928, Validation Loss: 0.6932\n",
      "Epoch 78 - Training Loss: 0.6798, Validation Loss: 0.6881\n",
      "Epoch 79 - Training Loss: 0.6844, Validation Loss: 0.6763\n",
      "Epoch 80 - Training Loss: 0.6840, Validation Loss: 0.6775\n",
      "Epoch 81 - Training Loss: 0.6919, Validation Loss: 0.6761\n",
      "Epoch 82 - Training Loss: 0.6882, Validation Loss: 0.6801\n",
      "Epoch 83 - Training Loss: 0.6814, Validation Loss: 0.6926\n",
      "Epoch 84 - Training Loss: 0.6783, Validation Loss: 0.6967\n",
      "Epoch 85 - Training Loss: 0.6784, Validation Loss: 0.6901\n",
      "Epoch 86 - Training Loss: 0.6810, Validation Loss: 0.6851\n",
      "Epoch 87 - Training Loss: 0.6884, Validation Loss: 0.6739\n",
      "Epoch 88 - Training Loss: 0.6831, Validation Loss: 0.6953\n",
      "Epoch 89 - Training Loss: 0.6897, Validation Loss: 0.6843\n",
      "Epoch 90 - Training Loss: 0.6817, Validation Loss: 0.6695\n",
      "Epoch 91 - Training Loss: 0.6808, Validation Loss: 0.6866\n",
      "Epoch 92 - Training Loss: 0.6858, Validation Loss: 0.6709\n",
      "Epoch 93 - Training Loss: 0.6840, Validation Loss: 0.6804\n",
      "Epoch 94 - Training Loss: 0.6780, Validation Loss: 0.6920\n",
      "Epoch 95 - Training Loss: 0.6848, Validation Loss: 0.6923\n",
      "Epoch 96 - Training Loss: 0.6929, Validation Loss: 0.6852\n",
      "Epoch 97 - Training Loss: 0.6837, Validation Loss: 0.6959\n",
      "Epoch 98 - Training Loss: 0.6801, Validation Loss: 0.6958\n",
      "Epoch 99 - Training Loss: 0.6870, Validation Loss: 0.6886\n",
      "Epoch 100 - Training Loss: 0.6991, Validation Loss: 0.6881\n",
      "Epoch 101 - Training Loss: 0.6801, Validation Loss: 0.6846\n",
      "Epoch 102 - Training Loss: 0.6913, Validation Loss: 0.6906\n",
      "Epoch 103 - Training Loss: 0.6825, Validation Loss: 0.7007\n",
      "Epoch 104 - Training Loss: 0.6854, Validation Loss: 0.6871\n",
      "Epoch 105 - Training Loss: 0.6830, Validation Loss: 0.6877\n",
      "Epoch 106 - Training Loss: 0.6766, Validation Loss: 0.6836\n",
      "Epoch 107 - Training Loss: 0.6742, Validation Loss: 0.6974\n",
      "Epoch 108 - Training Loss: 0.6890, Validation Loss: 0.6847\n",
      "Epoch 109 - Training Loss: 0.6723, Validation Loss: 0.6898\n",
      "Epoch 110 - Training Loss: 0.6897, Validation Loss: 0.6984\n",
      "Epoch 111 - Training Loss: 0.6743, Validation Loss: 0.6877\n",
      "Epoch 112 - Training Loss: 0.6903, Validation Loss: 0.7025\n",
      "Epoch 113 - Training Loss: 0.6779, Validation Loss: 0.6948\n",
      "Epoch 114 - Training Loss: 0.6838, Validation Loss: 0.6905\n",
      "Epoch 115 - Training Loss: 0.6915, Validation Loss: 0.6904\n",
      "Epoch 116 - Training Loss: 0.6808, Validation Loss: 0.6869\n",
      "Epoch 117 - Training Loss: 0.6773, Validation Loss: 0.6897\n",
      "Epoch 118 - Training Loss: 0.6705, Validation Loss: 0.6811\n",
      "Epoch 119 - Training Loss: 0.6833, Validation Loss: 0.6932\n",
      "Epoch 120 - Training Loss: 0.6848, Validation Loss: 0.6929\n",
      "Epoch 121 - Training Loss: 0.6870, Validation Loss: 0.6863\n",
      "Epoch 122 - Training Loss: 0.6867, Validation Loss: 0.6845\n",
      "Epoch 123 - Training Loss: 0.6797, Validation Loss: 0.6872\n",
      "Epoch 124 - Training Loss: 0.6718, Validation Loss: 0.6803\n",
      "Epoch 125 - Training Loss: 0.6751, Validation Loss: 0.6881\n",
      "Epoch 126 - Training Loss: 0.6929, Validation Loss: 0.6925\n",
      "Epoch 127 - Training Loss: 0.6854, Validation Loss: 0.6787\n",
      "Epoch 128 - Training Loss: 0.6722, Validation Loss: 0.6934\n",
      "Epoch 129 - Training Loss: 0.6757, Validation Loss: 0.6765\n",
      "Epoch 130 - Training Loss: 0.6703, Validation Loss: 0.6882\n",
      "Epoch 131 - Training Loss: 0.6678, Validation Loss: 0.6888\n",
      "Epoch 132 - Training Loss: 0.6851, Validation Loss: 0.6977\n",
      "Epoch 133 - Training Loss: 0.6685, Validation Loss: 0.6640\n",
      "Epoch 134 - Training Loss: 0.6694, Validation Loss: 0.6953\n",
      "Epoch 135 - Training Loss: 0.6785, Validation Loss: 0.6924\n",
      "Epoch 136 - Training Loss: 0.6776, Validation Loss: 0.6959\n",
      "Epoch 137 - Training Loss: 0.6793, Validation Loss: 0.6853\n",
      "Epoch 138 - Training Loss: 0.6771, Validation Loss: 0.7066\n",
      "Epoch 139 - Training Loss: 0.6893, Validation Loss: 0.6888\n",
      "Epoch 140 - Training Loss: 0.6813, Validation Loss: 0.6852\n",
      "Epoch 141 - Training Loss: 0.6771, Validation Loss: 0.6986\n",
      "Epoch 142 - Training Loss: 0.6799, Validation Loss: 0.6962\n",
      "Epoch 143 - Training Loss: 0.6885, Validation Loss: 0.6847\n",
      "Epoch 144 - Training Loss: 0.6875, Validation Loss: 0.6935\n",
      "Epoch 145 - Training Loss: 0.6806, Validation Loss: 0.6868\n",
      "Epoch 146 - Training Loss: 0.6699, Validation Loss: 0.6985\n",
      "Epoch 147 - Training Loss: 0.6846, Validation Loss: 0.6817\n",
      "Epoch 148 - Training Loss: 0.6732, Validation Loss: 0.6864\n",
      "Epoch 149 - Training Loss: 0.6816, Validation Loss: 0.6971\n",
      "Epoch 150 - Training Loss: 0.6680, Validation Loss: 0.6866\n",
      "Epoch 151 - Training Loss: 0.6767, Validation Loss: 0.6966\n",
      "Epoch 152 - Training Loss: 0.6856, Validation Loss: 0.6863\n",
      "Epoch 153 - Training Loss: 0.6837, Validation Loss: 0.6890\n",
      "Epoch 154 - Training Loss: 0.6821, Validation Loss: 0.6920\n",
      "Epoch 155 - Training Loss: 0.6772, Validation Loss: 0.6812\n",
      "Epoch 156 - Training Loss: 0.6830, Validation Loss: 0.6966\n",
      "Epoch 157 - Training Loss: 0.6831, Validation Loss: 0.6895\n",
      "Epoch 158 - Training Loss: 0.6723, Validation Loss: 0.6711\n",
      "Epoch 159 - Training Loss: 0.6676, Validation Loss: 0.6781\n",
      "Epoch 160 - Training Loss: 0.6864, Validation Loss: 0.6791\n",
      "Epoch 161 - Training Loss: 0.6768, Validation Loss: 0.6845\n",
      "Epoch 162 - Training Loss: 0.7168, Validation Loss: 0.6861\n",
      "Epoch 163 - Training Loss: 0.6909, Validation Loss: 0.7017\n",
      "Epoch 164 - Training Loss: 0.6788, Validation Loss: 0.6882\n",
      "Epoch 165 - Training Loss: 0.6711, Validation Loss: 0.6836\n",
      "Epoch 166 - Training Loss: 0.6893, Validation Loss: 0.6753\n",
      "Epoch 167 - Training Loss: 0.6769, Validation Loss: 0.6823\n",
      "Epoch 168 - Training Loss: 0.6844, Validation Loss: 0.6781\n",
      "Epoch 169 - Training Loss: 0.6784, Validation Loss: 0.6648\n",
      "Epoch 170 - Training Loss: 0.6743, Validation Loss: 0.6687\n",
      "Epoch 171 - Training Loss: 0.6787, Validation Loss: 0.6719\n",
      "Epoch 172 - Training Loss: 0.6830, Validation Loss: 0.6796\n",
      "Epoch 173 - Training Loss: 0.6725, Validation Loss: 0.6859\n",
      "Epoch 174 - Training Loss: 0.6779, Validation Loss: 0.6684\n",
      "Epoch 175 - Training Loss: 0.6882, Validation Loss: 0.6731\n",
      "Epoch 176 - Training Loss: 0.6779, Validation Loss: 0.6818\n",
      "Epoch 177 - Training Loss: 0.6777, Validation Loss: 0.6687\n",
      "Epoch 178 - Training Loss: 0.6809, Validation Loss: 0.6903\n",
      "Epoch 179 - Training Loss: 0.6907, Validation Loss: 0.6699\n",
      "Epoch 180 - Training Loss: 0.6716, Validation Loss: 0.6602\n",
      "Epoch 181 - Training Loss: 0.6870, Validation Loss: 0.6959\n",
      "Epoch 182 - Training Loss: 0.6858, Validation Loss: 0.6888\n",
      "Epoch 183 - Training Loss: 0.6850, Validation Loss: 0.6923\n",
      "Epoch 184 - Training Loss: 0.6766, Validation Loss: 0.6916\n",
      "Epoch 185 - Training Loss: 0.6771, Validation Loss: 0.6791\n",
      "Epoch 186 - Training Loss: 0.6732, Validation Loss: 0.6631\n",
      "Epoch 187 - Training Loss: 0.6904, Validation Loss: 0.6692\n",
      "Epoch 188 - Training Loss: 0.6809, Validation Loss: 0.6596\n",
      "Epoch 189 - Training Loss: 0.6855, Validation Loss: 0.6688\n",
      "Epoch 190 - Training Loss: 0.6915, Validation Loss: 0.6785\n",
      "Epoch 191 - Training Loss: 0.6815, Validation Loss: 0.6873\n",
      "Epoch 192 - Training Loss: 0.6818, Validation Loss: 0.6907\n",
      "Epoch 193 - Training Loss: 0.6772, Validation Loss: 0.6728\n",
      "Epoch 194 - Training Loss: 0.6795, Validation Loss: 0.6947\n",
      "Epoch 195 - Training Loss: 0.6811, Validation Loss: 0.6918\n",
      "Epoch 196 - Training Loss: 0.6927, Validation Loss: 0.6877\n",
      "Epoch 197 - Training Loss: 0.6925, Validation Loss: 0.6837\n",
      "Epoch 198 - Training Loss: 0.6817, Validation Loss: 0.6719\n",
      "Epoch 199 - Training Loss: 0.6823, Validation Loss: 0.6683\n",
      "Epoch 200 - Training Loss: 0.6800, Validation Loss: 0.6735\n",
      "Epoch 201 - Training Loss: 0.6688, Validation Loss: 0.6693\n",
      "Epoch 202 - Training Loss: 0.6659, Validation Loss: 0.6659\n",
      "Epoch 203 - Training Loss: 0.6723, Validation Loss: 0.6585\n",
      "Epoch 204 - Training Loss: 0.6755, Validation Loss: 0.6658\n",
      "Epoch 205 - Training Loss: 0.6934, Validation Loss: 0.6586\n",
      "Epoch 206 - Training Loss: 0.6890, Validation Loss: 0.6904\n",
      "Epoch 207 - Training Loss: 0.6889, Validation Loss: 0.7026\n",
      "Epoch 208 - Training Loss: 0.6782, Validation Loss: 0.6870\n",
      "Epoch 209 - Training Loss: 0.6721, Validation Loss: 0.6881\n",
      "Epoch 210 - Training Loss: 0.6800, Validation Loss: 0.6914\n",
      "Epoch 211 - Training Loss: 0.6879, Validation Loss: 0.6779\n",
      "Epoch 212 - Training Loss: 0.6797, Validation Loss: 0.6784\n",
      "Epoch 213 - Training Loss: 0.6854, Validation Loss: 0.6712\n",
      "Epoch 214 - Training Loss: 0.6756, Validation Loss: 0.6741\n",
      "Epoch 215 - Training Loss: 0.6786, Validation Loss: 0.6786\n",
      "Epoch 216 - Training Loss: 0.6864, Validation Loss: 0.6697\n",
      "Epoch 217 - Training Loss: 0.6736, Validation Loss: 0.6748\n",
      "Epoch 218 - Training Loss: 0.6796, Validation Loss: 0.6717\n",
      "Epoch 219 - Training Loss: 0.6758, Validation Loss: 0.6956\n",
      "Epoch 220 - Training Loss: 0.6751, Validation Loss: 0.6719\n",
      "Epoch 221 - Training Loss: 0.6786, Validation Loss: 0.6835\n",
      "Epoch 222 - Training Loss: 0.6739, Validation Loss: 0.6778\n",
      "Epoch 223 - Training Loss: 0.6783, Validation Loss: 0.6652\n",
      "Epoch 224 - Training Loss: 0.6820, Validation Loss: 0.6658\n",
      "Epoch 225 - Training Loss: 0.6778, Validation Loss: 0.6632\n",
      "Epoch 226 - Training Loss: 0.6663, Validation Loss: 0.6750\n",
      "Epoch 227 - Training Loss: 0.6737, Validation Loss: 0.6626\n",
      "Epoch 228 - Training Loss: 0.6839, Validation Loss: 0.6570\n",
      "Epoch 229 - Training Loss: 0.6755, Validation Loss: 0.6535\n",
      "Epoch 230 - Training Loss: 0.6765, Validation Loss: 0.6741\n",
      "Epoch 231 - Training Loss: 0.6771, Validation Loss: 0.6558\n",
      "Epoch 232 - Training Loss: 0.6689, Validation Loss: 0.6630\n",
      "Epoch 233 - Training Loss: 0.6816, Validation Loss: 0.6604\n",
      "Epoch 234 - Training Loss: 0.6775, Validation Loss: 0.6593\n",
      "Epoch 235 - Training Loss: 0.6723, Validation Loss: 0.6737\n",
      "Epoch 236 - Training Loss: 0.6750, Validation Loss: 0.6700\n",
      "Epoch 237 - Training Loss: 0.6729, Validation Loss: 0.6738\n",
      "Epoch 238 - Training Loss: 0.6706, Validation Loss: 0.6660\n",
      "Epoch 239 - Training Loss: 0.6693, Validation Loss: 0.6664\n",
      "Epoch 240 - Training Loss: 0.6632, Validation Loss: 0.6674\n",
      "Epoch 241 - Training Loss: 0.6647, Validation Loss: 0.6643\n",
      "Epoch 242 - Training Loss: 0.6701, Validation Loss: 0.6789\n",
      "Epoch 243 - Training Loss: 0.6686, Validation Loss: 0.6558\n",
      "Epoch 244 - Training Loss: 0.6711, Validation Loss: 0.6744\n",
      "Epoch 245 - Training Loss: 0.6736, Validation Loss: 0.6665\n",
      "Epoch 246 - Training Loss: 0.6816, Validation Loss: 0.6595\n",
      "Epoch 247 - Training Loss: 0.6731, Validation Loss: 0.6672\n",
      "Epoch 248 - Training Loss: 0.6879, Validation Loss: 0.6692\n",
      "Epoch 249 - Training Loss: 0.6702, Validation Loss: 0.6807\n",
      "Epoch 250 - Training Loss: 0.6753, Validation Loss: 0.6648\n",
      "Test predictions saved to C:\\Users\\didsu\\code_projects\\uni_24\\DL\\link_dl\\_02_homeworks\\homework_2\\test_predictions_ELU_testing_almost_final.csv\n",
      "Test predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Training Loss</td><td>▃▃▃█▂▃▃▂▃▂▂▃▂▂▂▃▂▁▂▂▁▂▂▂▂▂▂▁▂▂▂▂▃▂▁▂▂▂▁▁</td></tr><tr><td>Validation Loss</td><td>▄▅▆▅▇▆█▇▆▆▅▅▅▃▄▄▆▅▅▃▆▆▅▄▅▄▂▅▅▁▄▁▅▇▄▃▄▄▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>250</td></tr><tr><td>Training Loss</td><td>0.67535</td></tr><tr><td>Validation Loss</td><td>0.66484</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-armadillo-39</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/ynnkjb14' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training/runs/ynnkjb14</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_173251-ynnkjb14\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Section 7: Argument Parsing and Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # argparse.Namespace로 인자를 직접 설정합니다.\n",
    "    args = argparse.Namespace(\n",
    "        train_csv_path=\"train.csv\",                # 실제 훈련 데이터 경로\n",
    "        test_csv_path=\"test.csv\",                  # 실제 테스트 데이터 경로\n",
    "        output_path=\"test_predictions_ELU_testing_almost_final.csv\",    # 테스트 예측 결과 파일 경로\n",
    "        wandb=True,                                # WandB 사용 여부 (True로 설정하면 WandB 사용)\n",
    "        batch_size=32,                             # 배치 사이즈\n",
    "        epochs=250,                                # 학습 에폭 수\n",
    "        learning_rate=0.0005,                       # 학습률\n",
    "        hidden_units=[128, 64]                     # 은닉층 크기 리스트\n",
    "    )\n",
    "\n",
    "    # main 함수 호출\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67297f1-2922-4890-ae7a-4e806c0c7797",
   "metadata": {},
   "source": [
    "## 발견\n",
    "\n",
    "- Leaderboard의 경우 마지막 제출을 기준으로 판단하기도 함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4567c-56e7-4242-a2e4-48ea4f36f8fa",
   "metadata": {},
   "source": [
    "![competition78](./78Scores_leaderboard2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa175b-b114-4bf7-b850-dc3af2c315e6",
   "metadata": {},
   "source": [
    "## 에러 발생\n",
    "\n",
    "발견 - 하루 10개 이상 제출을 못함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6356477-f8df-46f7-9930-87510dc78365",
   "metadata": {},
   "source": [
    "![competition78](./Per_day_allow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f46031-d78a-4cef-9d14-9838dd77341b",
   "metadata": {},
   "source": [
    "# 소감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74d3bd-f5b8-489c-b186-0326ad379a6f",
   "metadata": {},
   "source": [
    "많은 시간을 투자하진 못했지만 그래도 짧은 시간에 최대한 시도해보려고 했던게 epoch를 200~400정도까지 바꿔가며 추세를 판단했습니다.\n",
    "\n",
    "batch 수는 한번만 하긴했지만 예를 들어 32라든가, learning rate도 0.0005로도 했던 것 같은데 시간이 오래걸리는만큼 세밀하게 학습할거라는 기대를 했지만 왜인지는 몰라도 그렇게 분석 정확도가 높지가 않던데 어떤식으로 해야 올라갈까를 고민하게된 계기가 되었으며 여러가지로 어떻게하면 올릴 수 있을까와. 어떤것을 올리면 어떤것도 영향을 끼칠까도 고민을 하고 시도하고 싶은 계기가 되었습니다. \n",
    "\n",
    "여러가지로 본인이 아는게 정말 ㅇ\n",
    "감사합니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
